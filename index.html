<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"wjq5712.github.io","root":"/","images":"/images","scheme":"Pisces","darkmode":true,"version":"8.17.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="https://wjq5712.github.io/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Wu Jiaqi">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://wjq5712.github.io/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"en","comments":"","permalink":"","path":"index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Hexo</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Hexo</h1>
      <i class="logo-line"></i>
    </a>
      <img class="custom-logo-image" src="https://raw.githubusercontent.com/wjq5712/BlogImages/main/logo.png" alt="Hexo">
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>







</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Wu Jiaqi</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">5</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://wjq5712.github.io/2023/06/15/BP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E5%8F%8AMATLAB%E7%BA%AF%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Wu Jiaqi">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/06/15/BP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E5%8F%8AMATLAB%E7%BA%AF%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/" class="post-title-link" itemprop="url">BP神经网络基本原理及MATLAB纯代码实现</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2023-06-15 11:04:18 / Modified: 11:07:11" itemprop="dateCreated datePublished" datetime="2023-06-15T11:04:18+08:00">2023-06-15</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="0-前言"><a href="#0-前言" class="headerlink" title="0 前言"></a>0 前言</h1><p>”神经网络就是个工具，我们只需要会用就行了。”</p>
<p>”底层原理我们不用懂，那是学计算机的研究的。”</p>
<p>“神经网络是科学也是玄学，调参就是看运气的。”</p>
<p>……</p>
<p>作为一个非计算机专业的学生，在涉及人工智能、深度学习和神经网络相关的问题时经常有人这么说，但事实果真如此么，我的答案是否定的。</p>
<p>神经网络好比一把枪，我们作为枪手看似只需扣动扳机、击发子弹、命中目标即可，但实际上我们在应用过程中会遇到各种各样的问题。比如，我该选择一个什么类型的网络？样本集该如何获取？数据如何预处理？参数如何调整？每个参数又是什么作用？过拟合是什么？欠拟合又是什么？激活函数怎么选？为什么我的网络总是训练失败？等等……</p>
<p>因此我决定在刚接触神经网络之时就打好基础，从简单的BP神经网络开始，学习基础原理，搞清楚每一个概念，并通过MATLAB纯代码的方式搭建一个BP神经网络，看看与工具箱有什么区别，为什么有区别。</p>
<p>值得一提的是，神经网络的种类浩如烟海，要把每一个的基本原理、结构拓扑都了然于胸确实不太现实，因此我的目标仅仅是通过BP这种简单的网络奠定一个基础，让自己心里有底。</p>
<h1 id="1-BP神经网络的模型简介"><a href="#1-BP神经网络的模型简介" class="headerlink" title="1 BP神经网络的模型简介"></a>1 BP神经网络的模型简介</h1><h2 id="1-1-BP神经网络结构"><a href="#1-1-BP神经网络结构" class="headerlink" title="1.1 BP神经网络结构"></a>1.1 BP神经网络结构</h2><p>BP神经网络一般性的拓扑图如下所示：</p>
<p><img src="https://raw.githubusercontent.com/wjq5712/BlogImages/main/202306151105991.jpeg" alt="111" style="zoom: 33%;" /></p>
<p>三明治结构，输入层-隐层-输出层，数据从输入层进入，输入层的各神经元将数据加权后传递至隐层的各个神经元，隐层神经元接收数据后根据自身阈值和激活函数加权传递给后续的神经元，指导最后一个隐层把数据传给输出层，最终由输出层神经元处理后输出结果。</p>
<p>仿生原理：人看到一只猫并辨认出的过程中，眼睛接收了输入，传递给了大脑的神经元，大脑的大量神经元经过复杂的处理后输出结果为：“这是一只猫”。神经元之间通过电信号传递，当电信号在神经元累计超过阈值后就会触发神经冲动，将电信号传给后续的神经元，人工神经网络正是仿照生物神经网络的原理构建起来的。</p>
<h2 id="1-2-神经元工作原理"><a href="#1-2-神经元工作原理" class="headerlink" title="1.2 神经元工作原理"></a>1.2 神经元工作原理</h2><p>每个神经元的工作原理如下：它将接收到的输入值加权求和后再加上阈值，经过激活函数的转换后输出到下一个神经元。</p>
<p><img src="https://raw.githubusercontent.com/wjq5712/BlogImages/main/202306102221775.jpg" alt="111" style="zoom:33%;" /></p>
<h2 id="1-3-BP神经网络的常用结构与配置"><a href="#1-3-BP神经网络的常用结构与配置" class="headerlink" title="1.3 BP神经网络的常用结构与配置"></a>1.3 BP神经网络的常用结构与配置</h2><p>神经网络的构建首先需要确定<strong>隐层的个数</strong>，<strong>每个隐层的神经元个数</strong>，<strong>每层神经元的激活函数</strong>。</p>
<p>通常，BP神经网络只会设置一个隐层，隐层激活函数设为tansig函数，输出层激活函数设为purelin。</p>
<p>tansig函数为S型函数：</p>
<script type="math/tex; mode=display">
y=\frac{2}{1+e^{-2x}}-1</script><p>其函数图像为：</p>
<p><img src="https://raw.githubusercontent.com/wjq5712/BlogImages/main/202306151105197.jpeg" alt="333" style="zoom: 67%;" /></p>
<p>也可用logsig函数（sigmoid）来代替tansig函数，区别在于logsig的取值范围为[0,1]，而tansig是[-1,1]。</p>
<script type="math/tex; mode=display">
y=\frac{1}{1+e^{-x}}</script><p>其函数图像为：</p>
<p><img src="https://raw.githubusercontent.com/wjq5712/BlogImages/main/202306102218546.jpg" alt="111" style="zoom: 67%;" /></p>
<p>purelin函数为恒等线性映射函数：</p>
<script type="math/tex; mode=display">
y=x</script><p>其函数图像为：</p>
<p><img src="https://raw.githubusercontent.com/wjq5712/BlogImages/main/202306151105841.jpeg" alt="111" style="zoom:67%;" /></p>
<p>单层网络结构简单，S型隐层激活函数提供了非线性拟合的能力，输出层采用purelin保证输出不受限制。</p>
<p>“麻雀虽小五脏俱全”</p>
<p>有人证明，只要隐层神经元足够多就能逼近任何函数。</p>
<h2 id="1-4-BP神经网络的数学表达式"><a href="#1-4-BP神经网络的数学表达式" class="headerlink" title="1.4 BP神经网络的数学表达式"></a>1.4 BP神经网络的数学表达式</h2><p>三层BP神经网络拓扑如下：</p>
<p><img src="https://raw.githubusercontent.com/wjq5712/BlogImages/main/202306151105587.jpeg" alt="111" style="zoom:33%;" /></p>
<ol>
<li>网络包含1个输入层，1个隐层，1个输出层；各层神经元数为[2,3,1]。</li>
<li>激活函数设置：隐层：tansig函数，输出层：purelin</li>
</ol>
<p>根据信号传递规则可以得出输入与输出之间的关系：</p>
<script type="math/tex; mode=display">
\begin{aligned}
y_1&=n_{31}\\
&=w_{21}^{31}n_{21}+w_{22}^{31}n_{22}+w_{23}^{31}n_{23}+b_{31} \\
&=w_{21}^{31}tansig(w_{11}^{21}n_{11}+w_{12}^{21}n_{12}+b_{21}) \\
&+w_{22}^{31}tansig(w_{11}^{22}n_{11}+w_{12}^{22}n_{12}+b_{22}) \\
&+w_{23}^{31}tansig(w_{11}^{23}n_{11}+w_{12}^{23}n_{12}+b_{23})+b_{31} \\
&=w_{21}^{31}tansig(w_{11}^{21}x_{1}+w_{12}^{21}x_{2}+b_{21}) \\
&+w_{22}^{31}tansig(w_{11}^{22}x_{1}+w_{12}^{22}x_{2}+b_{22}) \\
&+w_{23}^{31}tansig(w_{11}^{23}x_{1}+w_{12}^{23}x_{2}+b_{23}) \\
&+b_{31} 
\end{aligned}</script><p>其中，$w_{ij}^{pq}$表示第$i$层第$j$个节点到第$p$层第$q$个节点的权重，$b_{ij}$表示第$i$层第$j$个节点的阈值。</p>
<p>矩阵表示形式如下：</p>
<script type="math/tex; mode=display">
\pmb{y}=\pmb{f}(\pmb{x})=\pmb{W}^{(o)}tansig(\pmb{W}^{(h)}\pmb{x}+\pmb{b}^{(h)})+\pmb{b}^{(o)}
\label{exp}</script><p>其中，</p>
<script type="math/tex; mode=display">
\pmb{x} = 
\begin{bmatrix}
x_1\\
x_2
\end{bmatrix}</script><script type="math/tex; mode=display">
\pmb{y} = y_{1}</script><script type="math/tex; mode=display">
\pmb{W}^{(h)}=
\begin{bmatrix}
w_{11}^{21} \quad w_{12}^{21} \\
w_{11}^{22} \quad w_{12}^{22} \\
w_{11}^{23} \quad w_{12}^{23} \\
\end{bmatrix}</script><script type="math/tex; mode=display">
\pmb{b}^{(h)}=
\begin{bmatrix}
b_{21}\\
b_{22}\\
b_{23}
\end{bmatrix}</script><script type="math/tex; mode=display">
\pmb{W}^{(o)}=
\begin{bmatrix}
w_{21}^{31} \quad w_{22}^{31} \quad w_{23}^{31} 
\end{bmatrix}</script><script type="math/tex; mode=display">
\pmb{b}^{(o)} = b_{31}</script><p>更一般的情况，假设共有$n$层网络，第$i$层节点个数为$N_i$，则第$a$层到第$b$层的权重矩阵为</p>
<script type="math/tex; mode=display">
\pmb{W}_a^b=
\begin{bmatrix}
w_{a1}^{b1} \quad w_{a2}^{b1} \quad  \dots \quad w_{a1N_a}^{b1} \\
\vdots \\
w_{a1}^{bN_b} \quad w_{a2}^{bN_b} \quad  \dots \quad w_{a1N_a}^{bN_b} \\
\end{bmatrix}</script><p>阈值矩阵为</p>
<script type="math/tex; mode=display">
\pmb{b}_a^b=[b_1 \quad\dots\quad b_b]^\text{T}</script><h2 id="1-5-误差函数"><a href="#1-5-误差函数" class="headerlink" title="1.5 误差函数"></a>1.5 误差函数</h2><p>BP神经网络的误差函数采用预测值与真实值的均方误差，定义如下：</p>
<script type="math/tex; mode=display">
\pmb{E}(\pmb{W},\pmb{b})=\frac{1}{m}\sum_{i+1}^m \frac{1}{k}\sum_{j=1}^k(\hat{\pmb{y}}_{ij}-\pmb{y}_{ij})^2</script><p>其中，$m$为训练样本个数，$n$为输出个数。$\hat{\pmb{y}}_{ij}$为第$i$个样本的第$j$个输出的预测值，$\pmb{y}_{ij}$为对应的真实值。</p>
<p>显然，误差函数是一个关于权重矩阵和阈值矩阵的函数，采用不同的权重矩阵和阈值矩阵就对应不同的误差。</p>
<h2 id="1-6-BP神经网络的训练"><a href="#1-6-BP神经网络的训练" class="headerlink" title="1.6 BP神经网络的训练"></a>1.6 BP神经网络的训练</h2><p>常用的训练算法有梯度下降法、LM法，MATLAB提供的神经网络工具箱还采用了有动量的梯度下降法、自适应lr梯度下降法、自适应lr动量梯度下降法、弹性梯度下降法、Fletcher-Reeves共轭梯度法、Ploak-Ribiere共轭梯度法、Powell-Beale共轭梯度法、量化共轭梯度法、拟牛顿算法、一步正割算法、Levenberg-Marquardt法等。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">训练参数名</th>
<th style="text-align:center">训练方法</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">traingd</td>
<td style="text-align:center">梯度下降法</td>
</tr>
<tr>
<td style="text-align:center">traingdm</td>
<td style="text-align:center">有动量的梯度下降法</td>
</tr>
<tr>
<td style="text-align:center">traingda</td>
<td style="text-align:center">自适应lr梯度下降法</td>
</tr>
<tr>
<td style="text-align:center">traingdx</td>
<td style="text-align:center">自适应lr动量梯度下降法</td>
</tr>
<tr>
<td style="text-align:center">trainrp</td>
<td style="text-align:center">弹性梯度下降法</td>
</tr>
<tr>
<td style="text-align:center">traincgf</td>
<td style="text-align:center">Fletcher-Reeves共轭梯度法</td>
</tr>
<tr>
<td style="text-align:center">traincgp</td>
<td style="text-align:center">Ploak-Ribiere共轭梯度法</td>
</tr>
<tr>
<td style="text-align:center">traincgb</td>
<td style="text-align:center">Powell-Beale共轭梯度法</td>
</tr>
<tr>
<td style="text-align:center">trainscg</td>
<td style="text-align:center">量化共轭梯度法</td>
</tr>
<tr>
<td style="text-align:center">trainbfg</td>
<td style="text-align:center">拟牛顿算法</td>
</tr>
<tr>
<td style="text-align:center">trainoss</td>
<td style="text-align:center">一步正割算法</td>
</tr>
<tr>
<td style="text-align:center">trainlm</td>
<td style="text-align:center">Levenberg-Marquardt法</td>
</tr>
</tbody>
</table>
</div>
<h1 id="2-BP神经网络原理与本质"><a href="#2-BP神经网络原理与本质" class="headerlink" title="2 BP神经网络原理与本质"></a>2 BP神经网络原理与本质</h1><h2 id="2-1-隐层神经元的本质"><a href="#2-1-隐层神经元的本质" class="headerlink" title="2.1 隐层神经元的本质"></a>2.1 隐层神经元的本质</h2><p>从数学表达式不难看出，隐层神经元对应的数学对象就是tansig激活函数，而最终的输出$y$是由多个tansig函数叠加而成，因此本质上BP神经网络就是通过多个tansig函数叠加来组合成目标函数，用多少个隐层神经元就等于用了多少个tansig函数来拟合目标函数，因此理论上只要神经元个数足够多，单层BP神经网络就可以拟合出任何一个函数。</p>
<h2 id="2-2-参数的本质"><a href="#2-2-参数的本质" class="headerlink" title="2.2 参数的本质"></a>2.2 参数的本质</h2><ol>
<li>本层的权重绝定激活函数的“高矮”</li>
</ol>
<p><img src="https://raw.githubusercontent.com/wjq5712/BlogImages/main/202306110006746.jpg" alt="11" style="zoom: 67%;" /></p>
<ol>
<li>前一层的权重决定激活函数的“胖瘦”</li>
</ol>
<p><img src="https://raw.githubusercontent.com/wjq5712/BlogImages/main/202306151105710.jpeg" alt="22" style="zoom:67%;" /></p>
<ol>
<li>本层的阈值决定激活函数的“上下”</li>
</ol>
<p><img src="https://raw.githubusercontent.com/wjq5712/BlogImages/main/202306151105417.jpeg" alt="33" style="zoom:67%;" /></p>
<p>因此，参数的本质就是调整各个激活函数的形态，使得多个激活函数叠加后能毕竟目标函数。</p>
<p>总结：BP神经网络的层数和每层的神经元的个数决定了激活函数的个数，而各个神经元的参数（权重、阈值）决定了每个激活函数的形态，神经网络的训练就是找出最优的一组参数，使得激活函数的叠加逼近目标函数。</p>
<h2 id="2-3-误差逆向传播算法-errorBackPropagation"><a href="#2-3-误差逆向传播算法-errorBackPropagation" class="headerlink" title="2.3 误差逆向传播算法(errorBackPropagation)"></a>2.3 误差逆向传播算法(errorBackPropagation)</h2><p>误差逆向传播算法(BP算法)是应用最为广泛的一种学习算法，不仅可以用来训练多层前馈神经网络，还可用于其他类型的神经网络，如递归神经网络等。</p>
<p><img src="https://raw.githubusercontent.com/wjq5712/BlogImages/main/202306151105991.jpeg" alt="111" style="zoom: 33%;" /></p>
<p>假设输入层有$P$个节点，输入层信号向量为$\pmb{x}_{P \times 1}$，输出层有$Q$个节点，输出层信号向量为$\pmb{y}_{Q\times1}$，权重矩阵为$(\pmb{W}^\text{o})_{Q\times M_{N}}$，阈值矩阵为$(\pmb{b}^\text{o})_{Q \times 1}$，激活函数为$f^\text{o}(·)$，共有$N$层隐层，第$n$层隐层有$M_n$个节点，第$n$层隐层信号向量为$(\pmb{s}^n)_{M_{N} \times 1}$，第$n$层隐层的权重矩阵为$(\pmb{W}^n)_{M_n\times M_{n-1}}$，阈值矩阵为$(\pmb{b}^n)_{M_n \times 1}$，激活函数为$f^{n}(·)$。</p>
<p>根据式$\eqref{exp}$，可得一般化的多层BP神经网络的传递公式，</p>
<p>输出层的传递公式：</p>
<script type="math/tex; mode=display">
\pmb{y} = f^\text{o}(\pmb{W}^\text{o}\pmb{s}^{N}+\pmb{b}^o)</script><p>第$n$层隐层的传递公式：</p>
<script type="math/tex; mode=display">
\pmb{s}^n = f^n(\pmb{W}^n\pmb{s}^{n-1}+\pmb{b}^{n})</script><p>第1层隐层的传递公式为：</p>
<script type="math/tex; mode=display">
\pmb{s}^1=f^1(\pmb{W}^1\pmb{x}+\pmb{b}^1)</script><p>训练过程中，对样本$(\pmb{x}_k,\pmb{y}_k)$，神经网络输出的为预测值$\hat{\pmb{y}}_k$，则均方误差为：</p>
<script type="math/tex; mode=display">
E_k = \frac{1}{2} (\hat{\pmb{y}}_k-\pmb{y}_k)^\text{T}(\hat{\pmb{y}}_k-\pmb{y}_k)
\label{err}</script><p>BP神经网络基于梯度下降策略（Gradient Descent），以目标的负梯度方向对参数进行调整，对式$\eqref{err}$的误差$E_k$，给定学习率$\eta$，首先对输出层的参数进行迭代，为了避免对矩阵的求导，将权值矩阵$\pmb{W}_{m\times n}$变换为列向量，即</p>
<script type="math/tex; mode=display">
\tilde{\pmb{W}} = 
\begin{bmatrix}
W_{11} \quad W_{12} \quad \dots \quad W_{1n} \quad
W_{21} \quad W_{22} \quad \dots \quad W_{2n} \quad
\dots \quad
W_{m1} \quad W_{m2} \quad \dots \quad W_{mn} \quad
\end{bmatrix}^\text{T} \label{WeightMatTransf}</script><p>，传递公式变换为：</p>
<script type="math/tex; mode=display">
\pmb{y} = f^\text{o}(\pmb{S}^{N} \tilde{\pmb{W}}^\text{o}+\pmb{b}^{\text{o}})</script><p>其中，</p>
<script type="math/tex; mode=display">
\pmb{S}^{N} = 
\begin{bmatrix}
{\pmb{s}^{N}}^\text{T} \quad \pmb{0}_{1,M_{N}} \quad \dots \quad \pmb{0}_{1,M_{N}}\\
\pmb{0}_{1,M_{N}} \quad {\pmb{s}^{N}}^\text{T} \quad \dots \quad \pmb{0}_{1,M_{N}}\\
\vdots \\
\pmb{0}_{1,M_{N}} \quad \pmb{0}_{1,M_{N}} \quad \dots \quad {\pmb{s}^{N}}^\text{T}\\
\end{bmatrix}
=
\pmb{I}_{Q} \otimes \pmb{s}^{N}</script><p>$\otimes$表示克罗内克积。</p>
<p>则权值矩阵的修正量为，</p>
<script type="math/tex; mode=display">
\Delta \tilde{\pmb{W}}^\text{o} = -\eta \frac{\partial E_k}{\partial \tilde{\pmb{W}}^\text{o}}</script><p>其中，</p>
<script type="math/tex; mode=display">
\frac{\partial E_k}{\partial \tilde{\pmb{W}}^\text{o}} = 
(\frac{\partial \hat{\pmb{y}}_k}{\partial (\pmb{S}^{N} \tilde{\pmb{W}}^\text{o}+\pmb{b}^{\text{o}})}
\frac{\partial(\pmb{S}^{N} \tilde{\pmb{W}}^\text{o}+\pmb{b}^{\text{o}})}{\partial \tilde{\pmb{W}}^\text{o}})^\text{T}
\frac{\partial E_k}{\partial \hat{\pmb{y}}_k}</script><p>当激活函数为sigmoid时，利用其导数性质</p>
<script type="math/tex; mode=display">
f'_\text{sigmoid} = f_\text{sigmoid}(1-f_\text{sigmoid})</script><p>可得，</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial E_k}{\partial \tilde{\pmb{W}}^\text{o}} 
&= ((diag(\hat{\pmb{y}}_k)-\hat{\pmb{y}}_k\hat{\pmb{y}}_k^\text{T}){\pmb{S}^{N}})^\text{T} (\hat{\pmb{y}}_k-\pmb{y}_k) \\
&= {\pmb{S}^{N}}^\text{T}(diag(\hat{\pmb{y}}_k)-\hat{\pmb{y}}_k\hat{\pmb{y}}_k^\text{T})(\hat{\pmb{y}}_k-\pmb{y}_k)
\end{aligned}</script><p>阈值矩阵的修正量也类似，</p>
<script type="math/tex; mode=display">
\Delta {\pmb{b}}^\text{o} = -\eta \frac{\partial E_k}{\partial {\pmb{b}}^\text{o}}</script><p>其中，</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial E_k}{\partial \pmb{b}}^\text{o} &= 
(\frac{\partial \hat{\pmb{y}}_k}{\partial (\pmb{S}^{N} \tilde{\pmb{W}}^\text{o}+\pmb{b}^{\text{o}})}
\frac{\partial(\pmb{S}^{N} \tilde{\pmb{W}}^\text{o}+\pmb{b}^{\text{o}})}{\partial \pmb{b}^\text{o}})^\text{T}
\frac{\partial E_k}{\partial \hat{\pmb{y}}_k} \\
&= (diag(\hat{\pmb{y}}_k)-\hat{\pmb{y}}_k\hat{\pmb{y}}_k^\text{T})(\hat{\pmb{y}}_k-\pmb{y}_k)
\end{aligned}</script><p>不难发现，</p>
<script type="math/tex; mode=display">
\Delta \tilde{\pmb{W}}^\text{o} = {\pmb{S}^{N}}^\text{T} \Delta {\pmb{b}}^\text{o}</script><p>注意，当输出层的激活函数为purelin时，</p>
<script type="math/tex; mode=display">
\frac{\partial E_k}{\partial \tilde{\pmb{W}}^\text{o}} 
= {\pmb{S}^{N}}^\text{T}(\hat{\pmb{y}}_k-\pmb{y}_k)</script><script type="math/tex; mode=display">
\frac{\partial E_k}{\partial {\pmb{b}}^\text{o}} 
= (\hat{\pmb{y}}_k-\pmb{y}_k)</script><p>同样的，隐层的参数迭代也需要将权重矩阵$\pmb{W}^n$通过式$\eqref{WeightMatTransf}$变换为列向量，相应的修正量为：</p>
<script type="math/tex; mode=display">
\Delta \tilde{\pmb{W}}^n = -\eta \frac{\partial E_k}{\partial \tilde{\pmb{W}}^n}</script><p>其中，</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial E_k}{\partial \tilde{\pmb{W}}^n}
&=(\frac{\partial \hat{\pmb{y}}_k}{\partial \pmb{s}^{N}}
\frac{\partial \pmb{s}^{N}}{\partial \pmb{s}^{N-1}} \dots
\frac{\partial \pmb{s}^{n+1}}{\partial \pmb{s}^{n}}
\frac{\partial \pmb{s}^{n}}{\partial (\pmb{S}^{n-1} \tilde{\pmb{W}}^n+\pmb{b}^{n})}
\frac{\partial (\pmb{S}^{n-1} \tilde{\pmb{W}}^n+\pmb{b}^{n})}{\partial \tilde{\pmb{W}}^n})^\text{T}
\frac{\partial E_k}{\partial \hat{\pmb{y}}_k} \\
&=((diag(\hat{\pmb{y}}_k)-\hat{\pmb{y}}_k\hat{\pmb{y}}_k^\text{T}){\pmb{W}}^\text{o}
(diag(\pmb{s}^N)-\pmb{s}^N{\pmb{s}^N}^\text{T}){\pmb{W}}^N
(diag(\pmb{s}^{N-1})-\pmb{s}^{N-1}{\pmb{s}^{N-1}}^\text{T}){\pmb{W}}^{N-1} \dots
(diag(\pmb{s}^{n})-\pmb{s}^{n}{\pmb{s}^{n}}^\text{T})\pmb{S}^{n-1}
)^\text{T}(\hat{\pmb{y}}_k-\pmb{y}_k) \\
&= (\pmb{S}^{n-1})^\text{T} (diag(\pmb{s}^{n})-\pmb{s}^{n}{\pmb{s}^{n}}^\text{T})  
({\pmb{W}}^{n+1})^\text{T}(diag(\pmb{s}^{n+1})-\pmb{s}^{n+1}{\pmb{s}^{n+1}}^\text{T}) \dots 
({\pmb{W}}^{N})^\text{T}(diag(\pmb{s}^{N})-\pmb{s}^{N}{\pmb{s}^{N}}^\text{T})
({\pmb{W}}^\text{o})^\text{T}(diag(\hat{\pmb{y}}_k)-\hat{\pmb{y}}_k\hat{\pmb{y}}_k^\text{T})
(\hat{\pmb{y}}_k-\pmb{y}_k)
\end{aligned}</script><p>同理，</p>
<script type="math/tex; mode=display">
\begin{aligned}
\Delta {\pmb{b}}^n &= -\eta \frac{\partial E_k}{\partial {\pmb{b}}^n}\\
&= -\eta (diag(\pmb{s}^{n})-\pmb{s}^{n}{\pmb{s}^{n}}^\text{T})  
({\pmb{W}}^{n+1})^\text{T}(diag(\pmb{s}^{n+1})-\pmb{s}^{n+1}{\pmb{s}^{n+1}}^\text{T}) \dots 
({\pmb{W}}^{N})^\text{T}(diag(\pmb{s}^{N})-\pmb{s}^{N}{\pmb{s}^{N}}^\text{T})
({\pmb{W}}^\text{o})^\text{T}(diag(\hat{\pmb{y}}_k)-\hat{\pmb{y}}_k\hat{\pmb{y}}_k^\text{T})
(\hat{\pmb{y}}_k-\pmb{y}_k)
\end{aligned}</script><p>不难看出，</p>
<script type="math/tex; mode=display">
\Delta {\pmb{b}}^n=(diag(\pmb{s}^{n})-\pmb{s}^{n}{\pmb{s}^{n}}^\text{T})  
({\pmb{W}}^{n+1})^\text{T} \Delta {\pmb{b}}^{n+1}</script><script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial E_k}{\partial \tilde{\pmb{W}}^n}
&=(\pmb{S}^{n-1})^\text{T} (diag(\pmb{s}^{n})-\pmb{s}^{n}{\pmb{s}^{n}}^\text{T})  
(\pmb{W}^{n+1})^\text{T}(\pmb{S}^{n})^\text{-T}\frac{\partial E_k}{\partial \tilde{\pmb{W}}^{n+1}}\\
&=(\pmb{S}^{n-1})^\text{T} (diag(\pmb{s}^{n})-\pmb{s}^{n}{\pmb{s}^{n}}^\text{T})  
(\pmb{W}^{n+1})^\text{T}\frac{\partial E_k}{\partial {\pmb{b}}^n}
\end{aligned}</script><p>综上，BP神经网络训练过程中的参数修正量计算存在递推关系，计算顺序如下：</p>
<script type="math/tex; mode=display">
\hat{\pmb{y}}_k \rightarrow
\Delta {\pmb{b}}^\text{o} \rightarrow
\Delta \tilde{\pmb{W}}^\text{o} \rightarrow
\Delta {\pmb{b}}^\text{N} \rightarrow
\Delta \tilde{\pmb{W}}^\text{N} \rightarrow \dots
\Delta {\pmb{b}}^n \rightarrow
\Delta \tilde{\pmb{W}}^n \rightarrow \dots
\Delta {\pmb{b}}^1 \rightarrow
\Delta \tilde{\pmb{W}}^1</script><p>上式即BP神经网络的本质，信号从输入层进入经过隐层到输出端输出，对应了前馈网络的传播方向，训练时误差最先影响输出端的参数修正，再由后向前依次影响各隐层参数的修正，对应了误差的后向修正算法。</p>
<h1 id="3-代码实现"><a href="#3-代码实现" class="headerlink" title="3 代码实现"></a>3 代码实现</h1><p>实现了，但不想写了，有空再补:)</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://wjq5712.github.io/2023/06/10/%E5%85%B3%E4%BA%8E%E5%8A%A0%E9%80%9F%E5%BA%A6%E8%AE%A1%E6%B5%8B%E9%87%8F%E9%87%8F%E7%9A%84%E7%90%86%E8%A7%A3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Wu Jiaqi">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/06/10/%E5%85%B3%E4%BA%8E%E5%8A%A0%E9%80%9F%E5%BA%A6%E8%AE%A1%E6%B5%8B%E9%87%8F%E9%87%8F%E7%9A%84%E7%90%86%E8%A7%A3/" class="post-title-link" itemprop="url">关于加速度计测量量的理解</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2023-06-10 18:16:11 / Modified: 18:17:00" itemprop="dateCreated datePublished" datetime="2023-06-10T18:16:11+08:00">2023-06-10</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h1><p>加速度计传感器是用来测量飞行器加速度并输出加速度信号的装置，加速度传感器又称为加速度计，是惯性导航中的重要惯性元件。</p>
<p>加速度计与陀螺仪一样，测得的是本体系相对于惯性系的加速度在本体系下的分量（这句话存在着一定的问题，后面会解释）。</p>
<h1 id="2-加速度计模型"><a href="#2-加速度计模型" class="headerlink" title="2 加速度计模型"></a>2 加速度计模型</h1><p><img src="https://raw.githubusercontent.com/wjq5712/BlogImages/main/Pasted%2520image%252020230609165212.png" alt=""></p>
<p>加速度计的原理以认为是上图所示的模型，加速度计外壳（黄色部分）与刚体固连，加速度计外壳通过绳子与内部的小球相连。物体在自由移动时，<strong>小球对绳子的拉力的合力除以小球的质量，就是加速度计测量到的值</strong>。</p>
<h1 id="3-提出五个问题场景"><a href="#3-提出五个问题场景" class="headerlink" title="3 提出五个问题场景"></a>3 提出五个问题场景</h1><p>前提条件：设定向右为正方向，向上为正方向。</p>
<p>1、物体静止摆放到桌面上时，加速度计测量值是多少？<br>2、物体以10$m/s^2$的加速度向上运动时，加速度计测量值是多少？<br>3、物体自由落体时，加速度计的测量值是多少？<br>4、物体以10$m/s^2$向右运动时，加速度计在水平方向的测量值为多少？<br>5、物体绕轨道转动时，加速度计的测量值是多少？</p>
<p><img src="https://raw.githubusercontent.com/wjq5712/BlogImages/main/Pasted image 20230609172116.png" alt="Pasted image 20230609172116"></p>
<h1 id="4-对加速度计测量值的三种解释方法"><a href="#4-对加速度计测量值的三种解释方法" class="headerlink" title="4 对加速度计测量值的三种解释方法"></a>4 对加速度计测量值的三种解释方法</h1><h2 id="4-1-加速度计原理解释法"><a href="#4-1-加速度计原理解释法" class="headerlink" title="4.1 加速度计原理解释法"></a>4.1 加速度计原理解释法</h2><p>根据加速度计原理，物体小球对绳子的拉力除以小球的质量，就是加速度计测量得到的值，为了更好的建立力与加速度之间的关系，不妨设小球的质量为1kg，重力加速度设为10$m/s^2$则可以得到：</p>
<p>①静止状态下，小球对绳子的拉力等于小球的重力，因此为10N，方向向下，加速度计测得的值就是-10$m/s^2$。<br><img src="https://raw.githubusercontent.com/wjq5712/BlogImages/main/Pasted image 20230609172734.png" alt="Pasted image 20230609172734"></p>
<p>②物体以10$m/s^2$的加速度向上运动时，对球来说，球的加速度也是10$m/s^2$，因此对球运用牛顿第二定律。绳子对球的拉力为$F$。<br><img src="https://raw.githubusercontent.com/wjq5712/BlogImages/main/Pasted%2520image%252020230609173255.png" alt="123"></p>
<script type="math/tex; mode=display">
F-mg=ma</script><script type="math/tex; mode=display">
F=mg+ma=20N</script><p>则球对绳子的拉力$F’$为-20N，加速度计测得的值就为-20$m/s^2$。</p>
<p>③物体自由落体时，球的加速度为重力加速度，球只受重力，绳子上的力为0，加速度计测得的值也为0。</p>
<p><img src="https://raw.githubusercontent.com/wjq5712/BlogImages/main/Pasted image 20230609173851.png" alt="Pasted image 20230609173851"></p>
<p>④物体以10$m/s^2$向右运动时，球的加速度也为10$m/s^2$，建立牛顿第二定律方程，绳子对球的拉力为$F$。</p>
<p><img src="https://raw.githubusercontent.com/wjq5712/BlogImages/main/you.png" alt="you"></p>
<script type="math/tex; mode=display">
F=ma=10N</script><p>则球对绳子的拉力$F’$为-10N，加速度计测得的值就为-10$m/s^2$。</p>
<p>⑤物体在空间中绕地球轨道运动时，矢径方向，物体向心加速度为g，此时绳子的拉力为0，自然加速度计测得的值也为0；</p>
<p><img src="https://raw.githubusercontent.com/wjq5712/BlogImages/main/Pasted%20image%2020230609173906.png" alt="Pasted image 20230609173906"></p>
<p>注意：可以看到，①②③⑤这几种情况就与定义相冲突了，<strong>加速度计与陀螺仪一样，测得的是本体系相对于惯性系的加速度在本体系下的分量</strong>，但这几种情况下加速度计的实际测量值并代表物体在惯性系的得实际加速度，要想获得实际的加速度，必须要与物体的重力加速做一个组合，这种组合方式是什么样的呢？这就引入了第二种解释方式。</p>
<h2 id="4-2-惯性力系分解解释法"><a href="#4-2-惯性力系分解解释法" class="headerlink" title="4.2 惯性力系分解解释法"></a>4.2 惯性力系分解解释法</h2><p>以②号问题为例，当我们对小球做力系分析，可以得到：</p>
<script type="math/tex; mode=display">
F+mg=ma</script><p>此时力与加速度都是带符号的值，与坐标系正方向定义相关。</p>
<p>将所有项移到左边</p>
<script type="math/tex; mode=display">
F+mg-ma=0</script><p>式中$mg-ma$即为加速度计测量值。</p>
<p>将问题②带入上式：</p>
<script type="math/tex; mode=display">
mg-ma=-10-10=-20</script><p>加速度计测得的值就为-20$m/s^2$。</p>
<p>若想得到物体相对于惯性系实际的加速度，则有</p>
<script type="math/tex; mode=display">
a=\frac{mg-Value_{\text{加速度计测量}}}{m}=\frac{-10-(-20)}{1}=10m/s^2</script><p>其他问题同理，这样就建立了<strong>加速度计测量值</strong>与<strong>物体相对于惯性系的加速度</strong>两者之间的关系。</p>
<p>那为什么加速度计测量的值是$mg-ma$呢？这引入第三种解释方法</p>
<h2 id="4-3-相对加速度解释法"><a href="#4-3-相对加速度解释法" class="headerlink" title="4.3 相对加速度解释法"></a>4.3 相对加速度解释法</h2><p>同样以②号问题为例，我们认为加速度计外壳与小球是两个独立的个体，加速度计的测量值实际上是球相对于外壳的加速度。</p>
<p><img src="https://raw.githubusercontent.com/wjq5712/BlogImages/main/Pasted%2520image%252020230609175538.png" alt="Pasted image 20230609175538"></p>
<p>假设球漂浮在空中静止，当外壳以a=10$m/s^2$加速度向上运动时，球相对于壳的加速度就是-a=-10$m/s^2$，负号代表方向， 但球是会自由下落的，因此球相对于壳的加速度还要加上球本身下落的重力加速度，因此，加速度计测量的值就变成了$-a+g=-10+(-10)=-20$，自然也就解释了为什么$Value_{\text{加速度计测量}}=g-a$了。</p>
<h1 id="5-实验"><a href="#5-实验" class="headerlink" title="5 实验"></a>5 实验</h1><p>实际上，当我们使用真的加速度计模块时，确实是与理论解释是一致的。</p>
<p>读者可以尝试解释一下下图中物体的运动状态（物体只在垂直方向上运动）</p>
<p><img src="https://raw.githubusercontent.com/wjq5712/BlogImages/main/ea3f09a82bc028fb4ebb3ff93ab7e67.png" alt="ea3f09a82bc028fb4ebb3ff93ab7e67"></p>
<p>答案：物体先静止，然后向上加速，最后逐渐减速到速度为0，重新静止。注意</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://wjq5712.github.io/2023/06/10/Generalized-%CE%B1%E6%96%B9%E6%B3%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Wu Jiaqi">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/06/10/Generalized-%CE%B1%E6%96%B9%E6%B3%95/" class="post-title-link" itemprop="url">Generalized-α方法</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2023-06-10 17:02:40 / Modified: 17:04:00" itemprop="dateCreated datePublished" datetime="2023-06-10T17:02:40+08:00">2023-06-10</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Generalized-α方法"><a href="#Generalized-α方法" class="headerlink" title="Generalized-α方法"></a>Generalized-α方法</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Gemeralized-α为Newmark法的变种，其数值阻尼效果更具针对性，抑制高频，保留低频。</p>
<h2 id="动力学方程（二阶微分方程形式）"><a href="#动力学方程（二阶微分方程形式）" class="headerlink" title="动力学方程（二阶微分方程形式）"></a>动力学方程（二阶微分方程形式）</h2><script type="math/tex; mode=display">
\begin{cases}
\pmb{M}\ddot{\pmb{q}}+\pmb{\Phi}^\text{T}_{\pmb{q}}\pmb{\lambda}+\pmb{F}(\pmb{q})=\pmb{Q}(\pmb{q}) \\
\pmb{\Phi}(\pmb{q},t)=\pmb{0}
\end{cases}</script><h2 id="递推形式"><a href="#递推形式" class="headerlink" title="递推形式"></a>递推形式</h2><script type="math/tex; mode=display">
\pmb{q}_{n+1}  = \pmb{q}_n + h\dot{\pmb{q}_n} + h^2 (0.5-{\beta})\pmb{a}_n
+h^2 \beta \pmb{a}_{n+1} \\
  \dot{\pmb{q}}_{n+1}  = \dot{\pmb{q}}_n + h(1-\gamma)\pmb{a}_n + h \gamma \pmb{a}_{n+1} \\</script><p>式中，$h$为每个时间步长的大小，$\beta$、$\gamma$为算法参数。引入类加速度参数$\pmb{a}$</p>
<script type="math/tex; mode=display">
\begin{cases}
(1-\alpha_m)\pmb{a}_{n+1}+\alpha_m\pmb{a}_n = (1-\alpha_f)\ddot{\pmb{q}}_{n+1}+\alpha_f\ddot{\pmb{q}}_n\\
\pmb{a}_0=\ddot{\pmb{q}}_0
\end{cases}</script><p>算法参数为</p>
<script type="math/tex; mode=display">
\alpha_m = \frac{2\rho-1}{\rho+1} \\
\alpha_f = \frac{\rho}{\rho+1} \\
\beta = 0.25(1+\alpha_f-\alpha_m)^2\\
\gamma = 0.5 + \alpha_f - \alpha_m</script><p>这里的$\rho$的取值范围为$[0,1]$，决定了计算过程的能量耗散的大小。当$\rho=0$时，算法的能量耗散最大；当$\rho=1$时，算法不产生能量耗散。</p>
<h2 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a>算法实现</h2><p>step1：初始化状态量、广义α参数</p>
<p>step2：状态预测</p>
<script type="math/tex; mode=display">
\begin{aligned}
 \pmb{q}_{n+1}  &= \pmb{q}_n + h\dot{\pmb{q}_n} + h^2 (0.5-{\beta})\pmb{a}_n\\
 \dot{\pmb{q}}_{n+1}  &= \dot{\pmb{q}}_n + h(1-\gamma)\pmb{a}_n \\
 \pmb{\lambda}_{n+1} &= \pmb{0} \\
 \pmb{a}_{n+1} &= (\alpha_f \ddot{\pmb{q}}_n - \alpha_m \pmb{a}_n) / (1 - \alpha_m)\\
 \pmb{q}_{n+1}  &= \pmb{q}_{n+1}  + h^2 \beta \pmb{a}_{n+1} \\
 \dot{\pmb{q}}_{n+1}  &= \dot{\pmb{q}}_{n+1} + h \gamma \pmb{a}_{n+1}\\
 \ddot{\pmb{q}}_{n+1} &= \pmb{0}
\end{aligned}</script><p>step3：残差计算</p>
<script type="math/tex; mode=display">
\begin{aligned}
\pmb{r}_{\pmb{q}} &= \pmb{M}\ddot{\pmb{q}}_{n+1}+\pmb{\Phi}^\text{T}_{\pmb{q}}\pmb{\lambda}_{n+1}+\pmb{F}(\pmb{q}_{n+1})-\pmb{Q}(\pmb{q}_{n+1}) \\
\pmb{r}_{\pmb{\lambda}} &= \pmb{\Phi}(\pmb{q}_{n+1},t)
\end{aligned}</script><p>step4：残差矫正（牛顿-拉夫逊迭代）</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
\Delta\pmb{ q} \\
\Delta\pmb{ \lambda}
\end{bmatrix}
=
-\pmb{S}^{-1}_t
\begin{bmatrix}
\pmb{r_q} \\
\pmb{r_{\lambda}}
\end{bmatrix}</script><script type="math/tex; mode=display">
\pmb{q}_{n+1} = \pmb{q}_{n+1} + \Delta \pmb{ q} \\
\dot{\pmb{q}}_{n+1} = \dot{\pmb{q}}_{n+1} + \gamma' \Delta \pmb{ q} \\
\ddot{\pmb{q}}_{n+1} = \ddot{\pmb{q}}_{n+1} + \beta' \Delta \pmb{ q} \\
\pmb{\lambda}_{n+1} = \pmb{\lambda}_{n+1} + \Delta \pmb{ \lambda} \\</script><p>其中</p>
<script type="math/tex; mode=display">
\beta' = \frac{1-\alpha_m}{h^2\beta(1-\alpha_f)}\\
\gamma' = \frac{\gamma}{h\beta}\\

\pmb{S}_t = 
\begin{bmatrix}
(\beta ' \pmb{M} + \gamma ' \pmb{C}_t + \pmb{K}_t) & \pmb{\Phi}^\text{T}_{\pmb{q}} \\
\pmb{\Phi}_{\pmb{q}} & \pmb{0}
\end{bmatrix}</script><script type="math/tex; mode=display">
\pmb{K}_t = \frac{\partial (\pmb{M}\ddot{\pmb{q}}-\pmb{\Phi}^\text{T}_{\pmb{q}}\pmb{\lambda}+\pmb{F}(\pmb{q})-\pmb{Q}(\pmb{q}))}{\partial \pmb{q}}\\
\pmb{C}_t = \frac{\partial (\pmb{F}(\pmb{q})-\pmb{Q}(\pmb{q}))}{\partial \dot{\pmb{q}}}</script><p>step5：更新类加速度变量</p>
<script type="math/tex; mode=display">
\pmb{a}_{n+1} = \pmb{a}_{n+1} + \frac{1-\alpha_f}{1-\alpha_m} \ddot{\pmb{q}}_{n+1}</script><p>step6：回到step2循环迭代</p>
<p>流程图如下：</p>
<p><img src="https://raw.githubusercontent.com/wjq5712/BlogImages/main/image-20230510161115737.png" alt="image-20230510161115737" style="zoom:50%;" /></p>
<blockquote>
<p>[1] Arnold M, Brüls O. Convergence of the generalized-α scheme for constrained mechanical systems[J]. Multibody System Dynamics, 2007, 18: 185-202.</p>
</blockquote>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://wjq5712.github.io/2023/06/10/%E7%9F%A9%E9%98%B5%E7%9A%84%E5%AF%BC%E6%95%B0%E8%BF%90%E7%AE%97/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Wu Jiaqi">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/06/10/%E7%9F%A9%E9%98%B5%E7%9A%84%E5%AF%BC%E6%95%B0%E8%BF%90%E7%AE%97/" class="post-title-link" itemprop="url">矩阵的导数运算</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-06-10 16:58:07" itemprop="dateCreated datePublished" datetime="2023-06-10T16:58:07+08:00">2023-06-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-06-12 17:29:36" itemprop="dateModified" datetime="2023-06-12T17:29:36+08:00">2023-06-12</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="矩阵的导数运算"><a href="#矩阵的导数运算" class="headerlink" title="矩阵的导数运算"></a><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1av4y1b7MM/?spm_id_from=333.1007.top_right_bar_window_history.content.click">矩阵的导数运算</a></h1><h2 id="1-标量方程对向量的导数"><a href="#1-标量方程对向量的导数" class="headerlink" title="1. 标量方程对向量的导数"></a>1. 标量方程对向量的导数</h2><p>假设一个标量方程：</p>
<script type="math/tex; mode=display">
y=f(\pmb{x})</script><p>其中，$\boldsymbol{x}=[x_1\quad x_2 \quad \cdots \quad x_n]^T$。</p>
<p>$y$对$\pmb{x}$的导数有分母布局和分子布局两种形式。</p>
<p>分母布局（Denominator layout），偏导的行数与分母相同：</p>
<script type="math/tex; mode=display">
\frac{\partial y}{\partial \pmb{x}}=
\begin{bmatrix}
\frac{\partial y}{\partial x_1} \\
\frac{\partial y}{\partial x_2} \\
\vdots \\
\frac{\partial y}{\partial x_n}
\end{bmatrix}</script><p>分子布局（Numerator layout），偏导的行数与分子相同：</p>
<script type="math/tex; mode=display">
\frac{\partial y}{\partial \pmb{x}}=
\begin{bmatrix}
\frac{\partial y}{\partial x_1} \quad
\frac{\partial y}{\partial x_2} \quad
\cdots \quad
\frac{\partial y}{\partial x_n}
\end{bmatrix}</script><p>分子布局与分母布局的结果互为转置。</p>
<h2 id="2-标量方程对矩阵的导数"><a href="#2-标量方程对矩阵的导数" class="headerlink" title="2. 标量方程对矩阵的导数"></a>2. 标量方程对矩阵的导数</h2><p>假设一个标量方程：</p>
<script type="math/tex; mode=display">
y=f(\pmb{X})</script><p>其中，</p>
<script type="math/tex; mode=display">
\pmb{X}=
\begin{bmatrix}
X_{11} \quad X_{12} \quad \dots \quad X_{1n} \\
X_{21} \quad X_{22} \quad \dots \quad X_{2n} \\
\vdots
X_{m1} \quad X_{m2} \quad \dots \quad X_{mn} \\
\end{bmatrix}</script><p>则$y$对$\pmb{X}$的导数为：</p>
<script type="math/tex; mode=display">
\</script><h2 id="3-向量方程对向量的导数"><a href="#3-向量方程对向量的导数" class="headerlink" title="3. 向量方程对向量的导数"></a>3. 向量方程对向量的导数</h2><p>假设一个向量方程：</p>
<script type="math/tex; mode=display">
\pmb{y} = f(\pmb{x})</script><p>其中，$\pmb{x}=[x_1\quad x_2 \quad \cdots \quad x_n]^T$，$\pmb{y}=[y_1 \quad y_2 \quad \cdots \quad y_m]^T$。</p>
<p>分母布局下，$\pmb{y}$对$\pmb{x}$的导数为：</p>
<script type="math/tex; mode=display">
\frac{\partial \pmb{y}_{m \times 1}}{\partial \pmb{x}_{n \times 1}}=
\begin{bmatrix}
\frac{\partial \pmb{y}}{\partial x_1} \\
\frac{\partial \pmb{y}}{\partial x_2} \\
\vdots \\
\frac{\partial \pmb{y}}{\partial x_n} \\
\end{bmatrix}
=
\begin{bmatrix}
\frac{\partial y_1}{\partial x_1} \quad \frac{\partial y_2}{\partial x_1} \quad \dots \quad \frac{\partial y_m}{\partial x_1} \\
\frac{\partial y_1}{\partial x_2} \quad \frac{\partial y_2}{\partial x_2} \quad \dots \quad \frac{\partial y_m}{\partial x_2} \\
\vdots \\
\frac{\partial y_1}{\partial x_n} \quad \frac{\partial y_2}{\partial x_n} \quad \dots \quad \frac{\partial y_m}{\partial x_n} \\
\end{bmatrix}</script><p>第一步先使$\pmb{y}$分别对$x_1\dots \,x_n$求导，由于采用分母布局，因此结果为纵向展开，行数为$n$；第二步在对每一行继续按照分母布局展开，每一行的展开结果仍然是1行，列数为$m$。综上，分母布局下的最终求导结果为一个$m \times n$的矩阵。</p>
<p>分子布局同理，结果为分母布局的转置。结果如下：</p>
<script type="math/tex; mode=display">
\frac{\partial \pmb{y}_{m \times 1}}{\partial \pmb{x}_{n \times 1}}=
\begin{bmatrix}
\frac{\partial \pmb{y}}{\partial x_1} \\
\frac{\partial \pmb{y}}{\partial x_2} \\
\vdots \\
\frac{\partial \pmb{y}}{\partial x_n} \\
\end{bmatrix}
=
\begin{bmatrix}
\frac{\partial y_1}{\partial x_1} \quad \frac{\partial y_1}{\partial x_2} \quad \dots \quad \frac{\partial y_1}{\partial x_n} \\
\frac{\partial y_2}{\partial x_1} \quad \frac{\partial y_2}{\partial x_2} \quad \dots \quad \frac{\partial y_2}{\partial x_n} \\
\vdots \\
\frac{\partial y_m}{\partial x_1} \quad \frac{\partial y_m}{\partial x_2} \quad \dots \quad \frac{\partial y_m}{\partial x_n} \\
\end{bmatrix}</script><p>常用特例：</p>
<p>在分母布局下，假设$\pmb{x}=[x_1 \quad x_2 \quad \cdots \quad x_m]^T$，且</p>
<script type="math/tex; mode=display">
\pmb{A}=
\begin{bmatrix}
a_{11} \quad a_{12} \quad \dots \quad a_{1m} \\
a_{21} \quad a_{22} \quad \dots \quad a_{2m} \\
\vdots \\
a_{m1} \quad a_{m2} \quad \dots \quad a_{mm} \\
\end{bmatrix}</script><p>则</p>
<script type="math/tex; mode=display">
\frac{\partial \pmb{A} \pmb{x}}{\partial \pmb{x}} = \pmb{A}^\text{T}</script><p>而在<strong>分子布局</strong>下</p>
<script type="math/tex; mode=display">
\frac{\partial \pmb{A} \pmb{x}}{\partial \pmb{x}} = \pmb{A}</script><p>对二次型有：</p>
<script type="math/tex; mode=display">
\frac{\partial \pmb{x}^\text{T} \pmb{A} \pmb{x}}{\partial \pmb{x}}=\pmb{A}\pmb{x}+\pmb{A}^\text{T}\pmb{x}</script><p>特别的，当$A$为对称矩阵时，$\frac{\partial \pmb{x}^T \pmb{A} \pmb{x}}{\partial \pmb{x}}=2 \pmb{A}\pmb{x}$。</p>
<p><strong>注意：在公式推导的过程中必须要确定使用分母布局还是分子布局，混用两种布局会在编程仿真中出现矩阵维度不匹配的问题！！！</strong></p>
<h2 id="3-矩阵求导的链式法则"><a href="#3-矩阵求导的链式法则" class="headerlink" title="3. 矩阵求导的链式法则"></a>3. <a target="_blank" rel="noopener" href="https://www.cnblogs.com/pinard/p/10825264.html">矩阵求导的链式法则</a></h2><p><strong>约定：标量函数对向量、矩阵求导采用分母布局，向量函数对向量求导采用分子布局。</strong></p>
<p><strong>前者是为了保持求导结果与自变量向量维度相同，后者是为了与雅各比矩阵定义一致。</strong></p>
<h3 id="3-1-向量函数对向量求导的链式法则"><a href="#3-1-向量函数对向量求导的链式法则" class="headerlink" title="3.1 向量函数对向量求导的链式法则"></a>3.1 向量函数对向量求导的链式法则</h3><p>假设多个向量存在依赖关系，比如三个向量$\pmb{x} \rightarrow \pmb{y} \rightarrow \pmb{z}$，则有下面的链式求导法则：</p>
<script type="math/tex; mode=display">
\frac{\partial \pmb{z}}{\partial \pmb{x}} = \frac{\partial \pmb{z}}{\partial \pmb{y}} \frac{\partial \pmb{y}}{\partial \pmb{x}}</script><p>该法则可以推广到更多的向量依赖关系，但要注意依赖关系中所有的变量都是向量，如果其中存在矩阵或者标量，则该链式法则不成立。</p>
<p>假设有n个向量满足依赖关系：$\pmb{y_1} \rightarrow \pmb{y_1} \rightarrow \dots \rightarrow \pmb{y_n}$，则有下面的链式求导法则：</p>
<script type="math/tex; mode=display">
\frac{\partial \pmb{y_n}}{\partial \pmb{y_1}} = \frac{\partial \pmb{y_n}}{\partial \pmb{y_{n-1}}} \frac{\partial \pmb{y_{n-1}}}{\partial \pmb{y_{n-1}}} \dots \frac{\partial \pmb{y_2}}{\partial \pmb{y_1}}</script><h3 id="3-2-标量函数对向量求导的链式法则"><a href="#3-2-标量函数对向量求导的链式法则" class="headerlink" title="3.2 标量函数对向量求导的链式法则"></a>3.2 标量函数对向量求导的链式法则</h3><p>假设有一个标量$z$和两个向量$\pmb{x}、\pmb{y}$，它们满足依赖关系：$\pmb{x} \rightarrow \pmb{y} \rightarrow z$，则有下面的链式求导法则：</p>
<script type="math/tex; mode=display">
\frac{\partial z}{\partial \pmb{x}} = (\frac{\partial \pmb{y}}{\partial \pmb{x}})^\text{T} \frac{\partial z}{\partial \pmb{y}}</script><p>如果是标量函数对多个向量求导，比如：$\pmb{y_1} \rightarrow \pmb{y_1} \rightarrow \dots \rightarrow \pmb{y_n} \rightarrow z$，则其链式法则可表示为：</p>
<script type="math/tex; mode=display">
\frac{\partial z}{\partial \pmb{y_1}} = (\frac{\partial \pmb{y_n}}{\partial \pmb{y_{n-1}}} \frac{\partial \pmb{y_{n-1}}}{\partial \pmb{y_{n-1}}} \dots \frac{\partial \pmb{y_2}}{\partial \pmb{y_1}})^\text{T} \frac{\partial z}{\partial \pmb{y_n}}</script><h2 id="4-矩阵导数的乘法公式"><a href="#4-矩阵导数的乘法公式" class="headerlink" title="4. 矩阵导数的乘法公式"></a>4. 矩阵导数的乘法公式</h2><h3 id="4-1-标量×向量对标量求导"><a href="#4-1-标量×向量对标量求导" class="headerlink" title="4.1 标量×向量对标量求导"></a>4.1 标量×向量对标量求导</h3><p>假设有一个向量$\pmb{y}=u(x)\pmb{v}(x)$，其中$u(x)$是关于标量$x$的标量函数，$\pmb{v}(x)$是关于标量$x$的向量函数，则有如下求导公式：</p>
<script type="math/tex; mode=display">
\frac{\partial \pmb{y}} {\partial x} 
= 
\begin {bmatrix}
\frac{\partial y_1}{\partial x} \\
\vdots\\
\frac{\partial y_m}{\partial x}
\end{bmatrix}

=
\begin{bmatrix}
\frac{\partial u}{\partial x} v_1 + u \frac{\partial v_1}{\partial x} \\
\vdots \\
\frac{\partial u}{\partial x} v_m + u \frac{\partial v_m}{\partial x}
\end{bmatrix}
=
\frac{\partial u}{\partial x}\pmb{v} + u \frac{\partial \pmb{v}}{\partial x}</script><h3 id="4-2-标量×向量对向量求导"><a href="#4-2-标量×向量对向量求导" class="headerlink" title="4.2 标量×向量对向量求导"></a>4.2 标量×向量对向量求导</h3><p>假设有一个向量$\pmb{y}=u(\pmb{x})\pmb{v}(\pmb{x})$，其中$u(\pmb{x})$是关于标量$\pmb{x}$的向量函数，$\pmb{v}(\pmb{x})$是关于向量$\pmb{x}$的向量函数，则有如下求导公式：</p>
<script type="math/tex; mode=display">
\frac{\partial \pmb{y}} {\partial \pmb{x} }
= 
\begin {bmatrix}
\frac{\partial y_1}{\partial x_1} \dots \frac{\partial y_1}{\partial x_n} \\
\vdots \quad  \quad \quad  \vdots\\
\frac{\partial y_m}{\partial x_n} \dots \frac{\partial y_m}{\partial x_n} 
\end{bmatrix}
=
\begin{bmatrix}
\frac{\partial u}{\partial x_1} v_1 + u \frac{\partial v_1}{\partial x_1} \dots \frac{\partial u}{\partial x_n} v_1 + u \frac{\partial v_1}{\partial x_n}\\
\vdots \quad  \quad \quad \quad \quad \quad \quad  \vdots\\
\frac{\partial u}{\partial x_1} v_m + u \frac{\partial v_m}{\partial x_1} \dots \frac{\partial u}{\partial x_n} v_m + u \frac{\partial v_m}{\partial x_n}\\
\end{bmatrix}
=
\pmb{v} (\frac{\partial u}{\partial \pmb{x}})^\text{T} + u \frac{\partial \pmb{v}}{\partial \pmb{x}}</script><h2 id="5-矩阵导数的除法公式"><a href="#5-矩阵导数的除法公式" class="headerlink" title="5. 矩阵导数的除法公式"></a>5. 矩阵导数的除法公式</h2><h3 id="5-1-标量-向量对标量求导"><a href="#5-1-标量-向量对标量求导" class="headerlink" title="5.1 标量/向量对标量求导"></a>5.1 标量/向量对标量求导</h3><p>假设有一个向量$\pmb{y}=\pmb{u}({x})/{v}({x})$，其中$\pmb{u}(x)$是关于标量${x}$的向量函数，${v}({x})$是关于标量${x}$的标量函数，则有如下求导公式：</p>
<script type="math/tex; mode=display">
\frac{\partial \pmb{y}} {\partial x} 
= 
\begin {bmatrix}
\frac{\partial y_1}{\partial x} \\
\vdots\\
\frac{\partial y_m}{\partial x}
\end{bmatrix}

=
\begin{bmatrix}
\frac{1}{v^2}(\frac{\partial u}{\partial x} v_1 - u \frac{\partial v_1}{\partial x}) \\
\vdots \\
\frac{1}{v^2}(\frac{\partial u}{\partial x} v_m - u \frac{\partial v_m}{\partial x})
\end{bmatrix}
=
\frac{1}{v^2}( \frac{\partial u}{\partial x}\pmb{v} - u \frac{\partial \pmb{v}}{\partial x})</script><h3 id="5-2-标量-向量对向量求导"><a href="#5-2-标量-向量对向量求导" class="headerlink" title="5.2 标量/向量对向量求导"></a>5.2 标量/向量对向量求导</h3><p>假设有一个向量$\pmb{y}=\pmb{u}(\pmb{x})/{v}(\pmb{x})$，其中$\pmb{u}(\pmb{x})$是关于向量$\pmb{x}$的向量函数，${v}(\pmb{x})$是关于向量$\pmb{x}$的标量函数，则有如下求导公式：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial \pmb{y}} {\partial \pmb{x} }
&= 
\begin {bmatrix}
\frac{\partial y_1}{\partial x_1} \dots \frac{\partial y_1}{\partial x_n} \\
\vdots \quad  \quad \quad  \vdots\\
\frac{\partial y_m}{\partial x_n} \dots \frac{\partial y_m}{\partial x_n} 
\end{bmatrix}
=
\begin{bmatrix}
\frac{1}{v^2}(\frac{\partial u}{\partial x_1} v_1 - u \frac{\partial v_1}{\partial x_1})
\dots 
\frac{1}{v^2}(\frac{\partial u}{\partial x_n} v_1 - u \frac{\partial v_1}{\partial x_n}) \\
\vdots \quad  \quad \quad \quad \quad \quad \quad  \vdots\\
\frac{1}{v^2}(\frac{\partial u}{\partial x_1} v_m - u \frac{\partial v_m}{\partial x_1}) 
\dots 
\frac{1}{v^2}(\frac{\partial u}{\partial x_n} v_m - u \frac{\partial v_m}{\partial x_n}) \\
\end{bmatrix}\\
\\
&=
\frac{1}{v^2}({v}\frac{\partial \pmb{u}}{\partial \pmb{x}} - \pmb{u} (\frac{\partial {v}}{\partial \pmb{x}})^\text{T})
\end{aligned}</script>
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://wjq5712.github.io/2023/06/10/%E5%A7%BF%E6%80%81%E5%8A%A8%E5%8A%9B%E5%AD%A6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Wu Jiaqi">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/06/10/%E5%A7%BF%E6%80%81%E5%8A%A8%E5%8A%9B%E5%AD%A6/" class="post-title-link" itemprop="url">姿态动力学及控制</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2023-06-10 15:58:07 / Modified: 16:34:25" itemprop="dateCreated datePublished" datetime="2023-06-10T15:58:07+08:00">2023-06-10</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="姿态动力学及控制"><a href="#姿态动力学及控制" class="headerlink" title="姿态动力学及控制"></a>姿态动力学及控制</h1><h2 id="1-基于四元数的姿态动力学"><a href="#1-基于四元数的姿态动力学" class="headerlink" title="1 基于四元数的姿态动力学"></a>1 基于四元数的姿态动力学</h2><h3 id="1-1-四元数的定义"><a href="#1-1-四元数的定义" class="headerlink" title="1.1 四元数的定义"></a>1.1 四元数的定义</h3><p>任意两个坐标系之间的姿态转换关系都可以通过绕空间中一根轴旋转一个角度来描述，假设这根轴用一个单位向量$[e_1 \quad e_2 \quad e_3]^\text{T}$来表示，旋转的角度为$\theta$，<strong>四元数</strong>（又名欧拉参数），定义如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
& q_0 = e_1sin(\theta/2) \\
& q_1 = e_2sin(\theta/2) \\
& q_2 = e_3sin(\theta/2) \\
& q_3 = cos(\theta/2) \\
\end{aligned}</script><p>四元数$\pmb{q}=[q_0 \quad q_1 \quad q_2 \quad q_3]^\text{T}$，其中，$q_0$为标量部分，$\pmb{q}_r=[q_1 \quad q_2 \quad q_3]^\text{T}$为矢量部分。</p>
<h3 id="1-2-四元数描述的运动学"><a href="#1-2-四元数描述的运动学" class="headerlink" title="1.2 四元数描述的运动学"></a>1.2 四元数描述的运动学</h3><p>标量形式：</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
\dot{q}_0 \\ \dot{q}_1 \\ \dot{q}_2 \\ \dot{q}_3 \\ 
\end{bmatrix}
=
\frac{1}{2}
\begin{bmatrix}
&0 \quad &-\omega_1 \quad &-\omega_2 \quad &-\omega_3 \\
&\omega_1 \quad &0 \quad &\omega_3 \quad &-\omega_2 \\
&\omega_2 \quad & -\omega_3 \quad &0 \quad &\omega_1 \\
&\omega_3 \quad & \omega_2 \quad &-\omega_1 \quad &0
\end{bmatrix}
\begin{bmatrix}
{q}_0 \\ {q}_1 \\ {q}_2 \\ {q}_3 \\ 
\end{bmatrix}</script><p>矢量形式：</p>
<script type="math/tex; mode=display">
\begin{aligned}
& \dot q _0 = -\frac{1}{2} \pmb{\omega}^\text{T}\pmb{q}_r \\
& \dot{\pmb{q}}_r = \frac{1}{2} (q_0\pmb{\omega}-\pmb{\omega}^{\times}\pmb{q}_r)
\end{aligned}</script><p>其中，</p>
<script type="math/tex; mode=display">
\pmb{\omega}^{\times} = 
\begin{bmatrix}
& 0 \quad & -\omega_3 \quad & \omega_2 \\
& \omega_3 \quad & 0 \quad & -\omega_1 \\
& -\omega_2 \quad & \omega_1 \quad & 0
\end{bmatrix}</script><h3 id="1-3-四元数描述的运动学"><a href="#1-3-四元数描述的运动学" class="headerlink" title="1.3 四元数描述的运动学"></a>1.3 四元数描述的运动学</h3><script type="math/tex; mode=display">
\pmb{T} = \pmb{I}\dot{\pmb{\omega}}+\pmb{\omega}^{\times}\pmb{I\omega}</script><p>其中，$\pmb{T}$为刚体受到的合外力矩，$\pmb{\omega}$为刚体角速度（陀螺仪测得的本体系相对惯性系的角速度在本体系下的分量），$\pmb{I}$为刚体的转动惯量矩阵。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>





</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Wu Jiaqi</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  






  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>

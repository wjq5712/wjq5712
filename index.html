<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"wjq5712.github.io","root":"/","images":"/images","scheme":"Pisces","darkmode":true,"version":"8.17.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="https://wjq5712.github.io/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Wu Jiaqi">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://wjq5712.github.io/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"en","comments":"","permalink":"","path":"index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Hexo</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Hexo</h1>
      <i class="logo-line"></i>
    </a>
      <img class="custom-logo-image" src="https://raw.githubusercontent.com/wjq5712/BlogImages/main/logo.png" alt="Hexo">
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>







</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Wu Jiaqi</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">5</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://wjq5712.github.io/2023/06/15/BP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E5%8F%8AMATLAB%E7%BA%AF%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Wu Jiaqi">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/06/15/BP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E5%8F%8AMATLAB%E7%BA%AF%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/" class="post-title-link" itemprop="url">BP神经网络基本原理及MATLAB纯代码实现</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2023-06-15 11:04:18 / Modified: 11:07:11" itemprop="dateCreated datePublished" datetime="2023-06-15T11:04:18+08:00">2023-06-15</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="前言">0 前言</h1>
<p>”神经网络就是个工具，我们只需要会用就行了。”</p>
<p>”底层原理我们不用懂，那是学计算机的研究的。”</p>
<p>“神经网络是科学也是玄学，调参就是看运气的。”</p>
<p>……</p>
<p>作为一个非计算机专业的学生，在涉及人工智能、深度学习和神经网络相关的问题时经常有人这么说，但事实果真如此么，我的答案是否定的。</p>
<p>神经网络好比一把枪，我们作为枪手看似只需扣动扳机、击发子弹、命中目标即可，但实际上我们在应用过程中会遇到各种各样的问题。比如，我该选择一个什么类型的网络？样本集该如何获取？数据如何预处理？参数如何调整？每个参数又是什么作用？过拟合是什么？欠拟合又是什么？激活函数怎么选？为什么我的网络总是训练失败？等等……</p>
<p>因此我决定在刚接触神经网络之时就打好基础，从简单的BP神经网络开始，学习基础原理，搞清楚每一个概念，并通过MATLAB纯代码的方式搭建一个BP神经网络，看看与工具箱有什么区别，为什么有区别。</p>
<p>值得一提的是，神经网络的种类浩如烟海，要把每一个的基本原理、结构拓扑都了然于胸确实不太现实，因此我的目标仅仅是通过BP这种简单的网络奠定一个基础，让自己心里有底。</p>
<h1 id="bp神经网络的模型简介">1 BP神经网络的模型简介</h1>
<h2 id="bp神经网络结构">1.1 BP神经网络结构</h2>
<p>BP神经网络一般性的拓扑图如下所示：</p>
<p><img src="https://raw.githubusercontent.com/wjq5712/BlogImages/main/202306151105991.jpeg" alt="111" style="zoom: 33%;" /></p>
<p>三明治结构，输入层-隐层-输出层，数据从输入层进入，输入层的各神经元将数据加权后传递至隐层的各个神经元，隐层神经元接收数据后根据自身阈值和激活函数加权传递给后续的神经元，指导最后一个隐层把数据传给输出层，最终由输出层神经元处理后输出结果。</p>
<p>仿生原理：人看到一只猫并辨认出的过程中，眼睛接收了输入，传递给了大脑的神经元，大脑的大量神经元经过复杂的处理后输出结果为：“这是一只猫”。神经元之间通过电信号传递，当电信号在神经元累计超过阈值后就会触发神经冲动，将电信号传给后续的神经元，人工神经网络正是仿照生物神经网络的原理构建起来的。</p>
<h2 id="神经元工作原理">1.2 神经元工作原理</h2>
<p>每个神经元的工作原理如下：它将接收到的输入值加权求和后再加上阈值，经过激活函数的转换后输出到下一个神经元。</p>
<p><img src="https://raw.githubusercontent.com/wjq5712/BlogImages/main/202306102221775.jpg" alt="111" style="zoom:33%;" /></p>
<h2 id="bp神经网络的常用结构与配置">1.3 BP神经网络的常用结构与配置</h2>
<p>神经网络的构建首先需要确定<strong>隐层的个数</strong>，<strong>每个隐层的神经元个数</strong>，<strong>每层神经元的激活函数</strong>。</p>
<p>通常，BP神经网络只会设置一个隐层，隐层激活函数设为tansig函数，输出层激活函数设为purelin。</p>
<p>tansig函数为S型函数： <span class="math display">\[
y=\frac{2}{1+e^{-2x}}-1
\]</span> 其函数图像为：</p>
<p><img src="https://raw.githubusercontent.com/wjq5712/BlogImages/main/202306151105197.jpeg" alt="333" style="zoom: 67%;" /></p>
<p>也可用logsig函数（sigmoid）来代替tansig函数，区别在于logsig的取值范围为[0,1]，而tansig是[-1,1]。
<span class="math display">\[
y=\frac{1}{1+e^{-x}}
\]</span> 其函数图像为：</p>
<p><img src="https://raw.githubusercontent.com/wjq5712/BlogImages/main/202306102218546.jpg" alt="111" style="zoom: 67%;" /></p>
<p>purelin函数为恒等线性映射函数： <span class="math display">\[
y=x
\]</span> 其函数图像为：</p>
<p><img src="https://raw.githubusercontent.com/wjq5712/BlogImages/main/202306151105841.jpeg" alt="111" style="zoom:67%;" /></p>
<p>单层网络结构简单，S型隐层激活函数提供了非线性拟合的能力，输出层采用purelin保证输出不受限制。</p>
<p>“麻雀虽小五脏俱全”</p>
<p>有人证明，只要隐层神经元足够多就能逼近任何函数。</p>
<h2 id="bp神经网络的数学表达式">1.4 BP神经网络的数学表达式</h2>
<p>三层BP神经网络拓扑如下：</p>
<p><img src="https://raw.githubusercontent.com/wjq5712/BlogImages/main/202306151105587.jpeg" alt="111" style="zoom:33%;" /></p>
<ol type="1">
<li>网络包含1个输入层，1个隐层，1个输出层；各层神经元数为[2,3,1]。</li>
<li>激活函数设置：隐层：tansig函数，输出层：purelin</li>
</ol>
<p>根据信号传递规则可以得出输入与输出之间的关系： <span
class="math display">\[
\begin{aligned}
y_1&amp;=n_{31}\\
&amp;=w_{21}^{31}n_{21}+w_{22}^{31}n_{22}+w_{23}^{31}n_{23}+b_{31} \\
&amp;=w_{21}^{31}tansig(w_{11}^{21}n_{11}+w_{12}^{21}n_{12}+b_{21}) \\
&amp;+w_{22}^{31}tansig(w_{11}^{22}n_{11}+w_{12}^{22}n_{12}+b_{22}) \\
&amp;+w_{23}^{31}tansig(w_{11}^{23}n_{11}+w_{12}^{23}n_{12}+b_{23})+b_{31}
\\
&amp;=w_{21}^{31}tansig(w_{11}^{21}x_{1}+w_{12}^{21}x_{2}+b_{21}) \\
&amp;+w_{22}^{31}tansig(w_{11}^{22}x_{1}+w_{12}^{22}x_{2}+b_{22}) \\
&amp;+w_{23}^{31}tansig(w_{11}^{23}x_{1}+w_{12}^{23}x_{2}+b_{23}) \\
&amp;+b_{31}
\end{aligned}
\]</span> 其中，<span
class="math inline">\(w_{ij}^{pq}\)</span>表示第<span
class="math inline">\(i\)</span>层第<span
class="math inline">\(j\)</span>个节点到第<span
class="math inline">\(p\)</span>层第<span
class="math inline">\(q\)</span>个节点的权重，<span
class="math inline">\(b_{ij}\)</span>表示第<span
class="math inline">\(i\)</span>层第<span
class="math inline">\(j\)</span>个节点的阈值。</p>
<p>矩阵表示形式如下： <span class="math display">\[
\pmb{y}=\pmb{f}(\pmb{x})=\pmb{W}^{(o)}tansig(\pmb{W}^{(h)}\pmb{x}+\pmb{b}^{(h)})+\pmb{b}^{(o)}
\label{exp}
\]</span> 其中， <span class="math display">\[
\pmb{x} =
\begin{bmatrix}
x_1\\
x_2
\end{bmatrix}
\]</span></p>
<p><span class="math display">\[
\pmb{y} = y_{1}
\]</span></p>
<p><span class="math display">\[
\pmb{W}^{(h)}=
\begin{bmatrix}
w_{11}^{21} \quad w_{12}^{21} \\
w_{11}^{22} \quad w_{12}^{22} \\
w_{11}^{23} \quad w_{12}^{23} \\
\end{bmatrix}
\]</span></p>
<p><span class="math display">\[
\pmb{b}^{(h)}=
\begin{bmatrix}
b_{21}\\
b_{22}\\
b_{23}
\end{bmatrix}
\]</span></p>
<p><span class="math display">\[
\pmb{W}^{(o)}=
\begin{bmatrix}
w_{21}^{31} \quad w_{22}^{31} \quad w_{23}^{31}
\end{bmatrix}
\]</span></p>
<p><span class="math display">\[
\pmb{b}^{(o)} = b_{31}
\]</span></p>
<p>更一般的情况，假设共有<span
class="math inline">\(n\)</span>层网络，第<span
class="math inline">\(i\)</span>层节点个数为<span
class="math inline">\(N_i\)</span>，则第<span
class="math inline">\(a\)</span>层到第<span
class="math inline">\(b\)</span>层的权重矩阵为 <span
class="math display">\[
\pmb{W}_a^b=
\begin{bmatrix}
w_{a1}^{b1} \quad w_{a2}^{b1} \quad  \dots \quad w_{a1N_a}^{b1} \\
\vdots \\
w_{a1}^{bN_b} \quad w_{a2}^{bN_b} \quad  \dots \quad w_{a1N_a}^{bN_b} \\
\end{bmatrix}
\]</span> 阈值矩阵为 <span class="math display">\[
\pmb{b}_a^b=[b_1 \quad\dots\quad b_b]^\text{T}
\]</span></p>
<h2 id="误差函数">1.5 误差函数</h2>
<p>BP神经网络的误差函数采用预测值与真实值的均方误差，定义如下： <span
class="math display">\[
\pmb{E}(\pmb{W},\pmb{b})=\frac{1}{m}\sum_{i+1}^m
\frac{1}{k}\sum_{j=1}^k(\hat{\pmb{y}}_{ij}-\pmb{y}_{ij})^2
\]</span> 其中，<span
class="math inline">\(m\)</span>为训练样本个数，<span
class="math inline">\(n\)</span>为输出个数。<span
class="math inline">\(\hat{\pmb{y}}_{ij}\)</span>为第<span
class="math inline">\(i\)</span>个样本的第<span
class="math inline">\(j\)</span>个输出的预测值，<span
class="math inline">\(\pmb{y}_{ij}\)</span>为对应的真实值。</p>
<p>显然，误差函数是一个关于权重矩阵和阈值矩阵的函数，采用不同的权重矩阵和阈值矩阵就对应不同的误差。</p>
<h2 id="bp神经网络的训练">1.6 BP神经网络的训练</h2>
<p>常用的训练算法有梯度下降法、LM法，MATLAB提供的神经网络工具箱还采用了有动量的梯度下降法、自适应lr梯度下降法、自适应lr动量梯度下降法、弹性梯度下降法、Fletcher-Reeves共轭梯度法、Ploak-Ribiere共轭梯度法、Powell-Beale共轭梯度法、量化共轭梯度法、拟牛顿算法、一步正割算法、Levenberg-Marquardt法等。</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">训练参数名</th>
<th style="text-align: center;">训练方法</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">traingd</td>
<td style="text-align: center;">梯度下降法</td>
</tr>
<tr class="even">
<td style="text-align: center;">traingdm</td>
<td style="text-align: center;">有动量的梯度下降法</td>
</tr>
<tr class="odd">
<td style="text-align: center;">traingda</td>
<td style="text-align: center;">自适应lr梯度下降法</td>
</tr>
<tr class="even">
<td style="text-align: center;">traingdx</td>
<td style="text-align: center;">自适应lr动量梯度下降法</td>
</tr>
<tr class="odd">
<td style="text-align: center;">trainrp</td>
<td style="text-align: center;">弹性梯度下降法</td>
</tr>
<tr class="even">
<td style="text-align: center;">traincgf</td>
<td style="text-align: center;">Fletcher-Reeves共轭梯度法</td>
</tr>
<tr class="odd">
<td style="text-align: center;">traincgp</td>
<td style="text-align: center;">Ploak-Ribiere共轭梯度法</td>
</tr>
<tr class="even">
<td style="text-align: center;">traincgb</td>
<td style="text-align: center;">Powell-Beale共轭梯度法</td>
</tr>
<tr class="odd">
<td style="text-align: center;">trainscg</td>
<td style="text-align: center;">量化共轭梯度法</td>
</tr>
<tr class="even">
<td style="text-align: center;">trainbfg</td>
<td style="text-align: center;">拟牛顿算法</td>
</tr>
<tr class="odd">
<td style="text-align: center;">trainoss</td>
<td style="text-align: center;">一步正割算法</td>
</tr>
<tr class="even">
<td style="text-align: center;">trainlm</td>
<td style="text-align: center;">Levenberg-Marquardt法</td>
</tr>
</tbody>
</table>
<h1 id="bp神经网络原理与本质">2 BP神经网络原理与本质</h1>
<h2 id="隐层神经元的本质">2.1 隐层神经元的本质</h2>
<p>从数学表达式不难看出，隐层神经元对应的数学对象就是tansig激活函数，而最终的输出<span
class="math inline">\(y\)</span>是由多个tansig函数叠加而成，因此本质上BP神经网络就是通过多个tansig函数叠加来组合成目标函数，用多少个隐层神经元就等于用了多少个tansig函数来拟合目标函数，因此理论上只要神经元个数足够多，单层BP神经网络就可以拟合出任何一个函数。</p>
<h2 id="参数的本质">2.2 参数的本质</h2>
<ol type="1">
<li>本层的权重绝定激活函数的“高矮”</li>
</ol>
<p><img src="https://raw.githubusercontent.com/wjq5712/BlogImages/main/202306110006746.jpg" alt="11" style="zoom: 67%;" /></p>
<ol start="2" type="1">
<li>前一层的权重决定激活函数的“胖瘦”</li>
</ol>
<p><img src="https://raw.githubusercontent.com/wjq5712/BlogImages/main/202306151105710.jpeg" alt="22" style="zoom:67%;" /></p>
<ol start="3" type="1">
<li>本层的阈值决定激活函数的“上下”</li>
</ol>
<p><img src="https://raw.githubusercontent.com/wjq5712/BlogImages/main/202306151105417.jpeg" alt="33" style="zoom:67%;" /></p>
<p>因此，参数的本质就是调整各个激活函数的形态，使得多个激活函数叠加后能毕竟目标函数。</p>
<p>总结：BP神经网络的层数和每层的神经元的个数决定了激活函数的个数，而各个神经元的参数（权重、阈值）决定了每个激活函数的形态，神经网络的训练就是找出最优的一组参数，使得激活函数的叠加逼近目标函数。</p>
<h2 id="误差逆向传播算法errorbackpropagation">2.3
误差逆向传播算法(errorBackPropagation)</h2>
<p>误差逆向传播算法(BP算法)是应用最为广泛的一种学习算法，不仅可以用来训练多层前馈神经网络，还可用于其他类型的神经网络，如递归神经网络等。</p>
<p><img src="https://raw.githubusercontent.com/wjq5712/BlogImages/main/202306151105991.jpeg" alt="111" style="zoom: 33%;" /></p>
<p>假设输入层有<span
class="math inline">\(P\)</span>个节点，输入层信号向量为<span
class="math inline">\(\pmb{x}_{P \times 1}\)</span>，输出层有<span
class="math inline">\(Q\)</span>个节点，输出层信号向量为<span
class="math inline">\(\pmb{y}_{Q\times1}\)</span>，权重矩阵为<span
class="math inline">\((\pmb{W}^\text{o})_{Q\times
M_{N}}\)</span>，阈值矩阵为<span
class="math inline">\((\pmb{b}^\text{o})_{Q \times
1}\)</span>，激活函数为<span
class="math inline">\(f^\text{o}(·)\)</span>，共有<span
class="math inline">\(N\)</span>层隐层，第<span
class="math inline">\(n\)</span>层隐层有<span
class="math inline">\(M_n\)</span>个节点，第<span
class="math inline">\(n\)</span>层隐层信号向量为<span
class="math inline">\((\pmb{s}^n)_{M_{N} \times 1}\)</span>，第<span
class="math inline">\(n\)</span>层隐层的权重矩阵为<span
class="math inline">\((\pmb{W}^n)_{M_n\times
M_{n-1}}\)</span>，阈值矩阵为<span
class="math inline">\((\pmb{b}^n)_{M_n \times
1}\)</span>，激活函数为<span
class="math inline">\(f^{n}(·)\)</span>。</p>
<p>根据式<span
class="math inline">\(\eqref{exp}\)</span>，可得一般化的多层BP神经网络的传递公式，</p>
<p>输出层的传递公式： <span class="math display">\[
\pmb{y} = f^\text{o}(\pmb{W}^\text{o}\pmb{s}^{N}+\pmb{b}^o)
\]</span> 第<span class="math inline">\(n\)</span>层隐层的传递公式：
<span class="math display">\[
\pmb{s}^n = f^n(\pmb{W}^n\pmb{s}^{n-1}+\pmb{b}^{n})
\]</span> 第1层隐层的传递公式为： <span class="math display">\[
\pmb{s}^1=f^1(\pmb{W}^1\pmb{x}+\pmb{b}^1)
\]</span> 训练过程中，对样本<span
class="math inline">\((\pmb{x}_k,\pmb{y}_k)\)</span>，神经网络输出的为预测值<span
class="math inline">\(\hat{\pmb{y}}_k\)</span>，则均方误差为： <span
class="math display">\[
E_k = \frac{1}{2}
(\hat{\pmb{y}}_k-\pmb{y}_k)^\text{T}(\hat{\pmb{y}}_k-\pmb{y}_k)
\label{err}
\]</span> BP神经网络基于梯度下降策略（Gradient
Descent），以目标的负梯度方向对参数进行调整，对式<span
class="math inline">\(\eqref{err}\)</span>的误差<span
class="math inline">\(E_k\)</span>，给定学习率<span
class="math inline">\(\eta\)</span>，首先对输出层的参数进行迭代，为了避免对矩阵的求导，将权值矩阵<span
class="math inline">\(\pmb{W}_{m\times n}\)</span>变换为列向量，即 <span
class="math display">\[
\tilde{\pmb{W}} =
\begin{bmatrix}
W_{11} \quad W_{12} \quad \dots \quad W_{1n} \quad
W_{21} \quad W_{22} \quad \dots \quad W_{2n} \quad
\dots \quad
W_{m1} \quad W_{m2} \quad \dots \quad W_{mn} \quad
\end{bmatrix}^\text{T} \label{WeightMatTransf}
\]</span> ，传递公式变换为： <span class="math display">\[
\pmb{y} = f^\text{o}(\pmb{S}^{N}
\tilde{\pmb{W}}^\text{o}+\pmb{b}^{\text{o}})
\]</span> 其中， <span class="math display">\[
\pmb{S}^{N} =
\begin{bmatrix}
{\pmb{s}^{N}}^\text{T} \quad \pmb{0}_{1,M_{N}} \quad \dots \quad
\pmb{0}_{1,M_{N}}\\
\pmb{0}_{1,M_{N}} \quad {\pmb{s}^{N}}^\text{T} \quad \dots \quad
\pmb{0}_{1,M_{N}}\\
\vdots \\
\pmb{0}_{1,M_{N}} \quad \pmb{0}_{1,M_{N}} \quad \dots \quad
{\pmb{s}^{N}}^\text{T}\\
\end{bmatrix}
=
\pmb{I}_{Q} \otimes \pmb{s}^{N}
\]</span> <span
class="math inline">\(\otimes\)</span>表示克罗内克积。</p>
<p>则权值矩阵的修正量为， <span class="math display">\[
\Delta \tilde{\pmb{W}}^\text{o} = -\eta \frac{\partial E_k}{\partial
\tilde{\pmb{W}}^\text{o}}
\]</span> 其中， <span class="math display">\[
\frac{\partial E_k}{\partial \tilde{\pmb{W}}^\text{o}} =
(\frac{\partial \hat{\pmb{y}}_k}{\partial (\pmb{S}^{N}
\tilde{\pmb{W}}^\text{o}+\pmb{b}^{\text{o}})}
\frac{\partial(\pmb{S}^{N}
\tilde{\pmb{W}}^\text{o}+\pmb{b}^{\text{o}})}{\partial
\tilde{\pmb{W}}^\text{o}})^\text{T}
\frac{\partial E_k}{\partial \hat{\pmb{y}}_k}
\]</span> 当激活函数为sigmoid时，利用其导数性质 <span
class="math display">\[
f&#39;_\text{sigmoid} = f_\text{sigmoid}(1-f_\text{sigmoid})
\]</span> 可得，</p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial E_k}{\partial \tilde{\pmb{W}}^\text{o}}
&amp;=
((diag(\hat{\pmb{y}}_k)-\hat{\pmb{y}}_k\hat{\pmb{y}}_k^\text{T}){\pmb{S}^{N}})^\text{T}
(\hat{\pmb{y}}_k-\pmb{y}_k) \\
&amp;=
{\pmb{S}^{N}}^\text{T}(diag(\hat{\pmb{y}}_k)-\hat{\pmb{y}}_k\hat{\pmb{y}}_k^\text{T})(\hat{\pmb{y}}_k-\pmb{y}_k)
\end{aligned}
\]</span> 阈值矩阵的修正量也类似， <span class="math display">\[
\Delta {\pmb{b}}^\text{o} = -\eta \frac{\partial E_k}{\partial
{\pmb{b}}^\text{o}}
\]</span> 其中， <span class="math display">\[
\begin{aligned}
\frac{\partial E_k}{\partial \pmb{b}}^\text{o} &amp;=
(\frac{\partial \hat{\pmb{y}}_k}{\partial (\pmb{S}^{N}
\tilde{\pmb{W}}^\text{o}+\pmb{b}^{\text{o}})}
\frac{\partial(\pmb{S}^{N}
\tilde{\pmb{W}}^\text{o}+\pmb{b}^{\text{o}})}{\partial
\pmb{b}^\text{o}})^\text{T}
\frac{\partial E_k}{\partial \hat{\pmb{y}}_k} \\
&amp;=
(diag(\hat{\pmb{y}}_k)-\hat{\pmb{y}}_k\hat{\pmb{y}}_k^\text{T})(\hat{\pmb{y}}_k-\pmb{y}_k)
\end{aligned}
\]</span> 不难发现， <span class="math display">\[
\Delta \tilde{\pmb{W}}^\text{o} = {\pmb{S}^{N}}^\text{T} \Delta
{\pmb{b}}^\text{o}
\]</span> 注意，当输出层的激活函数为purelin时， <span
class="math display">\[
\frac{\partial E_k}{\partial \tilde{\pmb{W}}^\text{o}}
= {\pmb{S}^{N}}^\text{T}(\hat{\pmb{y}}_k-\pmb{y}_k)
\]</span></p>
<p><span class="math display">\[
\frac{\partial E_k}{\partial {\pmb{b}}^\text{o}}
= (\hat{\pmb{y}}_k-\pmb{y}_k)
\]</span></p>
<p>同样的，隐层的参数迭代也需要将权重矩阵<span
class="math inline">\(\pmb{W}^n\)</span>通过式<span
class="math inline">\(\eqref{WeightMatTransf}\)</span>变换为列向量，相应的修正量为：
<span class="math display">\[
\Delta \tilde{\pmb{W}}^n = -\eta \frac{\partial E_k}{\partial
\tilde{\pmb{W}}^n}
\]</span> 其中， <span class="math display">\[
\begin{aligned}
\frac{\partial E_k}{\partial \tilde{\pmb{W}}^n}
&amp;=(\frac{\partial \hat{\pmb{y}}_k}{\partial \pmb{s}^{N}}
\frac{\partial \pmb{s}^{N}}{\partial \pmb{s}^{N-1}} \dots
\frac{\partial \pmb{s}^{n+1}}{\partial \pmb{s}^{n}}
\frac{\partial \pmb{s}^{n}}{\partial (\pmb{S}^{n-1}
\tilde{\pmb{W}}^n+\pmb{b}^{n})}
\frac{\partial (\pmb{S}^{n-1} \tilde{\pmb{W}}^n+\pmb{b}^{n})}{\partial
\tilde{\pmb{W}}^n})^\text{T}
\frac{\partial E_k}{\partial \hat{\pmb{y}}_k} \\
&amp;=((diag(\hat{\pmb{y}}_k)-\hat{\pmb{y}}_k\hat{\pmb{y}}_k^\text{T}){\pmb{W}}^\text{o}
(diag(\pmb{s}^N)-\pmb{s}^N{\pmb{s}^N}^\text{T}){\pmb{W}}^N
(diag(\pmb{s}^{N-1})-\pmb{s}^{N-1}{\pmb{s}^{N-1}}^\text{T}){\pmb{W}}^{N-1}
\dots
(diag(\pmb{s}^{n})-\pmb{s}^{n}{\pmb{s}^{n}}^\text{T})\pmb{S}^{n-1}
)^\text{T}(\hat{\pmb{y}}_k-\pmb{y}_k) \\
&amp;= (\pmb{S}^{n-1})^\text{T}
(diag(\pmb{s}^{n})-\pmb{s}^{n}{\pmb{s}^{n}}^\text{T})  
({\pmb{W}}^{n+1})^\text{T}(diag(\pmb{s}^{n+1})-\pmb{s}^{n+1}{\pmb{s}^{n+1}}^\text{T})
\dots
({\pmb{W}}^{N})^\text{T}(diag(\pmb{s}^{N})-\pmb{s}^{N}{\pmb{s}^{N}}^\text{T})
({\pmb{W}}^\text{o})^\text{T}(diag(\hat{\pmb{y}}_k)-\hat{\pmb{y}}_k\hat{\pmb{y}}_k^\text{T})
(\hat{\pmb{y}}_k-\pmb{y}_k)
\end{aligned}
\]</span> 同理， <span class="math display">\[
\begin{aligned}
\Delta {\pmb{b}}^n &amp;= -\eta \frac{\partial E_k}{\partial
{\pmb{b}}^n}\\
&amp;= -\eta (diag(\pmb{s}^{n})-\pmb{s}^{n}{\pmb{s}^{n}}^\text{T})  
({\pmb{W}}^{n+1})^\text{T}(diag(\pmb{s}^{n+1})-\pmb{s}^{n+1}{\pmb{s}^{n+1}}^\text{T})
\dots
({\pmb{W}}^{N})^\text{T}(diag(\pmb{s}^{N})-\pmb{s}^{N}{\pmb{s}^{N}}^\text{T})
({\pmb{W}}^\text{o})^\text{T}(diag(\hat{\pmb{y}}_k)-\hat{\pmb{y}}_k\hat{\pmb{y}}_k^\text{T})
(\hat{\pmb{y}}_k-\pmb{y}_k)
\end{aligned}
\]</span></p>
<p>不难看出， <span class="math display">\[
\Delta
{\pmb{b}}^n=(diag(\pmb{s}^{n})-\pmb{s}^{n}{\pmb{s}^{n}}^\text{T})  
({\pmb{W}}^{n+1})^\text{T} \Delta {\pmb{b}}^{n+1}
\]</span></p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial E_k}{\partial \tilde{\pmb{W}}^n}
&amp;=(\pmb{S}^{n-1})^\text{T}
(diag(\pmb{s}^{n})-\pmb{s}^{n}{\pmb{s}^{n}}^\text{T})  
(\pmb{W}^{n+1})^\text{T}(\pmb{S}^{n})^\text{-T}\frac{\partial
E_k}{\partial \tilde{\pmb{W}}^{n+1}}\\
&amp;=(\pmb{S}^{n-1})^\text{T}
(diag(\pmb{s}^{n})-\pmb{s}^{n}{\pmb{s}^{n}}^\text{T})  
(\pmb{W}^{n+1})^\text{T}\frac{\partial E_k}{\partial {\pmb{b}}^n}
\end{aligned}
\]</span>
综上，BP神经网络训练过程中的参数修正量计算存在递推关系，计算顺序如下：
<span class="math display">\[
\hat{\pmb{y}}_k \rightarrow
\Delta {\pmb{b}}^\text{o} \rightarrow
\Delta \tilde{\pmb{W}}^\text{o} \rightarrow
\Delta {\pmb{b}}^\text{N} \rightarrow
\Delta \tilde{\pmb{W}}^\text{N} \rightarrow \dots
\Delta {\pmb{b}}^n \rightarrow
\Delta \tilde{\pmb{W}}^n \rightarrow \dots
\Delta {\pmb{b}}^1 \rightarrow
\Delta \tilde{\pmb{W}}^1
\]</span>
上式即BP神经网络的本质，信号从输入层进入经过隐层到输出端输出，对应了前馈网络的传播方向，训练时误差最先影响输出端的参数修正，再由后向前依次影响各隐层参数的修正，对应了误差的后向修正算法。</p>
<h1 id="代码实现">3 代码实现</h1>
<p>实现了，但不想写了，有空再补:)</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://wjq5712.github.io/2023/06/10/%E5%85%B3%E4%BA%8E%E5%8A%A0%E9%80%9F%E5%BA%A6%E8%AE%A1%E6%B5%8B%E9%87%8F%E9%87%8F%E7%9A%84%E7%90%86%E8%A7%A3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Wu Jiaqi">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/06/10/%E5%85%B3%E4%BA%8E%E5%8A%A0%E9%80%9F%E5%BA%A6%E8%AE%A1%E6%B5%8B%E9%87%8F%E9%87%8F%E7%9A%84%E7%90%86%E8%A7%A3/" class="post-title-link" itemprop="url">关于加速度计测量量的理解</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2023-06-10 18:16:11 / Modified: 18:17:00" itemprop="dateCreated datePublished" datetime="2023-06-10T18:16:11+08:00">2023-06-10</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="简介">1 简介</h1>
<p>加速度计传感器是用来测量飞行器加速度并输出加速度信号的装置，加速度传感器又称为加速度计，是惯性导航中的重要惯性元件。</p>
<p>加速度计与陀螺仪一样，测得的是本体系相对于惯性系的加速度在本体系下的分量（这句话存在着一定的问题，后面会解释）。</p>
<h1 id="加速度计模型">2 加速度计模型</h1>
<p><img
src="https://raw.githubusercontent.com/wjq5712/BlogImages/main/Pasted%2520image%252020230609165212.png" /></p>
<p>加速度计的原理以认为是上图所示的模型，加速度计外壳（黄色部分）与刚体固连，加速度计外壳通过绳子与内部的小球相连。物体在自由移动时，<strong>小球对绳子的拉力的合力除以小球的质量，就是加速度计测量到的值</strong>。</p>
<h1 id="提出五个问题场景">3 提出五个问题场景</h1>
<p>前提条件：设定向右为正方向，向上为正方向。</p>
<p>1、物体静止摆放到桌面上时，加速度计测量值是多少？ 2、物体以10<span
class="math inline">\(m/s^2\)</span>的加速度向上运动时，加速度计测量值是多少？
3、物体自由落体时，加速度计的测量值是多少？ 4、物体以10<span
class="math inline">\(m/s^2\)</span>向右运动时，加速度计在水平方向的测量值为多少？
5、物体绕轨道转动时，加速度计的测量值是多少？</p>
<figure>
<img
src="https://raw.githubusercontent.com/wjq5712/BlogImages/main/Pasted%20image%2020230609172116.png"
alt="Pasted image 20230609172116" />
<figcaption aria-hidden="true">Pasted image 20230609172116</figcaption>
</figure>
<h1 id="对加速度计测量值的三种解释方法">4
对加速度计测量值的三种解释方法</h1>
<h2 id="加速度计原理解释法">4.1 加速度计原理解释法</h2>
<p>根据加速度计原理，物体小球对绳子的拉力除以小球的质量，就是加速度计测量得到的值，为了更好的建立力与加速度之间的关系，不妨设小球的质量为1kg，重力加速度设为10<span
class="math inline">\(m/s^2\)</span>则可以得到：</p>
<p>①静止状态下，小球对绳子的拉力等于小球的重力，因此为10N，方向向下，加速度计测得的值就是-10<span
class="math inline">\(m/s^2\)</span>。 <img
src="https://raw.githubusercontent.com/wjq5712/BlogImages/main/Pasted%20image%2020230609172734.png"
alt="Pasted image 20230609172734" /></p>
<p>②物体以10<span
class="math inline">\(m/s^2\)</span>的加速度向上运动时，对球来说，球的加速度也是10<span
class="math inline">\(m/s^2\)</span>，因此对球运用牛顿第二定律。绳子对球的拉力为<span
class="math inline">\(F\)</span>。 <img
src="https://raw.githubusercontent.com/wjq5712/BlogImages/main/Pasted%2520image%252020230609173255.png"
alt="123" /> <span class="math display">\[
F-mg=ma
\]</span></p>
<p><span class="math display">\[
F=mg+ma=20N
\]</span></p>
<p>则球对绳子的拉力<span
class="math inline">\(F&#39;\)</span>为-20N，加速度计测得的值就为-20<span
class="math inline">\(m/s^2\)</span>。</p>
<p>③物体自由落体时，球的加速度为重力加速度，球只受重力，绳子上的力为0，加速度计测得的值也为0。</p>
<figure>
<img
src="https://raw.githubusercontent.com/wjq5712/BlogImages/main/Pasted%20image%2020230609173851.png"
alt="Pasted image 20230609173851" />
<figcaption aria-hidden="true">Pasted image 20230609173851</figcaption>
</figure>
<p>④物体以10<span
class="math inline">\(m/s^2\)</span>向右运动时，球的加速度也为10<span
class="math inline">\(m/s^2\)</span>，建立牛顿第二定律方程，绳子对球的拉力为<span
class="math inline">\(F\)</span>。</p>
<p><img
src="https://raw.githubusercontent.com/wjq5712/BlogImages/main/you.png"
alt="you" /> <span class="math display">\[
F=ma=10N
\]</span> 则球对绳子的拉力<span
class="math inline">\(F&#39;\)</span>为-10N，加速度计测得的值就为-10<span
class="math inline">\(m/s^2\)</span>。</p>
<p>⑤物体在空间中绕地球轨道运动时，矢径方向，物体向心加速度为g，此时绳子的拉力为0，自然加速度计测得的值也为0；</p>
<figure>
<img
src="https://raw.githubusercontent.com/wjq5712/BlogImages/main/Pasted%20image%2020230609173906.png"
alt="Pasted image 20230609173906" />
<figcaption aria-hidden="true">Pasted image 20230609173906</figcaption>
</figure>
<p>注意：可以看到，①②③⑤这几种情况就与定义相冲突了，<strong>加速度计与陀螺仪一样，测得的是本体系相对于惯性系的加速度在本体系下的分量</strong>，但这几种情况下加速度计的实际测量值并代表物体在惯性系的得实际加速度，要想获得实际的加速度，必须要与物体的重力加速做一个组合，这种组合方式是什么样的呢？这就引入了第二种解释方式。</p>
<h2 id="惯性力系分解解释法">4.2 惯性力系分解解释法</h2>
<p>以②号问题为例，当我们对小球做力系分析，可以得到：</p>
<p><span class="math display">\[
F+mg=ma
\]</span> 此时力与加速度都是带符号的值，与坐标系正方向定义相关。</p>
<p>将所有项移到左边</p>
<p><span class="math display">\[
F+mg-ma=0
\]</span> 式中<span
class="math inline">\(mg-ma\)</span>即为加速度计测量值。</p>
<p>将问题②带入上式：</p>
<p><span class="math display">\[
mg-ma=-10-10=-20
\]</span> 加速度计测得的值就为-20<span
class="math inline">\(m/s^2\)</span>。</p>
<p>若想得到物体相对于惯性系实际的加速度，则有 <span
class="math display">\[
a=\frac{mg-Value_{\text{加速度计测量}}}{m}=\frac{-10-(-20)}{1}=10m/s^2
\]</span>
其他问题同理，这样就建立了<strong>加速度计测量值</strong>与<strong>物体相对于惯性系的加速度</strong>两者之间的关系。</p>
<p>那为什么加速度计测量的值是<span
class="math inline">\(mg-ma\)</span>呢？这引入第三种解释方法</p>
<h2 id="相对加速度解释法">4.3 相对加速度解释法</h2>
<p>同样以②号问题为例，我们认为加速度计外壳与小球是两个独立的个体，加速度计的测量值实际上是球相对于外壳的加速度。</p>
<figure>
<img
src="https://raw.githubusercontent.com/wjq5712/BlogImages/main/Pasted%2520image%252020230609175538.png"
alt="Pasted image 20230609175538" />
<figcaption aria-hidden="true">Pasted image 20230609175538</figcaption>
</figure>
<p>假设球漂浮在空中静止，当外壳以a=10<span
class="math inline">\(m/s^2\)</span>加速度向上运动时，球相对于壳的加速度就是-a=-10<span
class="math inline">\(m/s^2\)</span>，负号代表方向，
但球是会自由下落的，因此球相对于壳的加速度还要加上球本身下落的重力加速度，因此，加速度计测量的值就变成了<span
class="math inline">\(-a+g=-10+(-10)=-20\)</span>，自然也就解释了为什么<span
class="math inline">\(Value_{\text{加速度计测量}}=g-a\)</span>了。</p>
<h1 id="实验">5 实验</h1>
<p>实际上，当我们使用真的加速度计模块时，确实是与理论解释是一致的。</p>
<p>读者可以尝试解释一下下图中物体的运动状态（物体只在垂直方向上运动）</p>
<figure>
<img
src="https://raw.githubusercontent.com/wjq5712/BlogImages/main/ea3f09a82bc028fb4ebb3ff93ab7e67.png"
alt="ea3f09a82bc028fb4ebb3ff93ab7e67" />
<figcaption
aria-hidden="true">ea3f09a82bc028fb4ebb3ff93ab7e67</figcaption>
</figure>
<p>答案：物体先静止，然后向上加速，最后逐渐减速到速度为0，重新静止。注意</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://wjq5712.github.io/2023/06/10/Generalized-%CE%B1%E6%96%B9%E6%B3%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Wu Jiaqi">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/06/10/Generalized-%CE%B1%E6%96%B9%E6%B3%95/" class="post-title-link" itemprop="url">Generalized-α方法</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2023-06-10 17:02:40 / Modified: 17:04:00" itemprop="dateCreated datePublished" datetime="2023-06-10T17:02:40+08:00">2023-06-10</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="generalized-α方法">Generalized-α方法</h1>
<h2 id="简介">简介</h2>
<p>Gemeralized-α为Newmark法的变种，其数值阻尼效果更具针对性，抑制高频，保留低频。</p>
<h2 id="动力学方程二阶微分方程形式">动力学方程（二阶微分方程形式）</h2>
<p><span class="math display">\[
\begin{cases}
\pmb{M}\ddot{\pmb{q}}+\pmb{\Phi}^\text{T}_{\pmb{q}}\pmb{\lambda}+\pmb{F}(\pmb{q})=\pmb{Q}(\pmb{q})
\\
\pmb{\Phi}(\pmb{q},t)=\pmb{0}
\end{cases}
\]</span></p>
<h2 id="递推形式">递推形式</h2>
<p><span class="math display">\[
\pmb{q}_{n+1}  = \pmb{q}_n + h\dot{\pmb{q}_n} + h^2
(0.5-{\beta})\pmb{a}_n
+h^2 \beta \pmb{a}_{n+1} \\
  \dot{\pmb{q}}_{n+1}  = \dot{\pmb{q}}_n + h(1-\gamma)\pmb{a}_n + h
\gamma \pmb{a}_{n+1} \\
\]</span></p>
<p>式中，<span
class="math inline">\(h\)</span>为每个时间步长的大小，<span
class="math inline">\(\beta\)</span>、<span
class="math inline">\(\gamma\)</span>为算法参数。引入类加速度参数<span
class="math inline">\(\pmb{a}\)</span> <span class="math display">\[
\begin{cases}
(1-\alpha_m)\pmb{a}_{n+1}+\alpha_m\pmb{a}_n =
(1-\alpha_f)\ddot{\pmb{q}}_{n+1}+\alpha_f\ddot{\pmb{q}}_n\\
\pmb{a}_0=\ddot{\pmb{q}}_0
\end{cases}
\]</span> 算法参数为 <span class="math display">\[
\alpha_m = \frac{2\rho-1}{\rho+1} \\
\alpha_f = \frac{\rho}{\rho+1} \\
\beta = 0.25(1+\alpha_f-\alpha_m)^2\\
\gamma = 0.5 + \alpha_f - \alpha_m
\]</span> 这里的<span
class="math inline">\(\rho\)</span>的取值范围为<span
class="math inline">\([0,1]\)</span>，决定了计算过程的能量耗散的大小。当<span
class="math inline">\(\rho=0\)</span>时，算法的能量耗散最大；当<span
class="math inline">\(\rho=1\)</span>时，算法不产生能量耗散。</p>
<h2 id="算法实现">算法实现</h2>
<p>step1：初始化状态量、广义α参数</p>
<p>step2：状态预测 <span class="math display">\[
\begin{aligned}
\pmb{q}_{n+1}  &amp;= \pmb{q}_n + h\dot{\pmb{q}_n} + h^2
(0.5-{\beta})\pmb{a}_n\\
\dot{\pmb{q}}_{n+1}  &amp;= \dot{\pmb{q}}_n + h(1-\gamma)\pmb{a}_n \\
\pmb{\lambda}_{n+1} &amp;= \pmb{0} \\
\pmb{a}_{n+1} &amp;= (\alpha_f \ddot{\pmb{q}}_n - \alpha_m \pmb{a}_n) /
(1 - \alpha_m)\\
\pmb{q}_{n+1}  &amp;= \pmb{q}_{n+1}  + h^2 \beta \pmb{a}_{n+1} \\
\dot{\pmb{q}}_{n+1}  &amp;= \dot{\pmb{q}}_{n+1} + h \gamma
\pmb{a}_{n+1}\\
\ddot{\pmb{q}}_{n+1} &amp;= \pmb{0}
\end{aligned}
\]</span> step3：残差计算 <span class="math display">\[
\begin{aligned}
\pmb{r}_{\pmb{q}} &amp;=
\pmb{M}\ddot{\pmb{q}}_{n+1}+\pmb{\Phi}^\text{T}_{\pmb{q}}\pmb{\lambda}_{n+1}+\pmb{F}(\pmb{q}_{n+1})-\pmb{Q}(\pmb{q}_{n+1})
\\
\pmb{r}_{\pmb{\lambda}} &amp;= \pmb{\Phi}(\pmb{q}_{n+1},t)
\end{aligned}
\]</span> step4：残差矫正（牛顿-拉夫逊迭代） <span
class="math display">\[
\begin{bmatrix}
\Delta\pmb{ q} \\
\Delta\pmb{ \lambda}
\end{bmatrix}
=
-\pmb{S}^{-1}_t
\begin{bmatrix}
\pmb{r_q} \\
\pmb{r_{\lambda}}
\end{bmatrix}
\]</span></p>
<p><span class="math display">\[
\pmb{q}_{n+1} = \pmb{q}_{n+1} + \Delta \pmb{ q} \\
\dot{\pmb{q}}_{n+1} = \dot{\pmb{q}}_{n+1} + \gamma&#39; \Delta \pmb{ q}
\\
\ddot{\pmb{q}}_{n+1} = \ddot{\pmb{q}}_{n+1} + \beta&#39; \Delta \pmb{ q}
\\
\pmb{\lambda}_{n+1} = \pmb{\lambda}_{n+1} + \Delta \pmb{ \lambda} \\
\]</span></p>
<p>其中 $$ ' = \ ' = \</p>
_t =
<span class="math display">\[\begin{bmatrix}
(\beta &#39; \pmb{M} + \gamma &#39; \pmb{C}_t + \pmb{K}_t) &amp;
\pmb{\Phi}^\text{T}_{\pmb{q}} \\
\pmb{\Phi}_{\pmb{q}} &amp; \pmb{0}
\end{bmatrix}\]</span>
<p>$$</p>
<p><span class="math display">\[
\pmb{K}_t = \frac{\partial
(\pmb{M}\ddot{\pmb{q}}-\pmb{\Phi}^\text{T}_{\pmb{q}}\pmb{\lambda}+\pmb{F}(\pmb{q})-\pmb{Q}(\pmb{q}))}{\partial
\pmb{q}}\\
\pmb{C}_t = \frac{\partial (\pmb{F}(\pmb{q})-\pmb{Q}(\pmb{q}))}{\partial
\dot{\pmb{q}}}
\]</span></p>
<p>step5：更新类加速度变量 <span class="math display">\[
\pmb{a}_{n+1} = \pmb{a}_{n+1} + \frac{1-\alpha_f}{1-\alpha_m}
\ddot{\pmb{q}}_{n+1}
\]</span> step6：回到step2循环迭代</p>
<p>流程图如下：</p>
<p><img src="https://raw.githubusercontent.com/wjq5712/BlogImages/main/image-20230510161115737.png" alt="image-20230510161115737" style="zoom:50%;" /></p>
<blockquote>
<p>[1] Arnold M, Brüls O. Convergence of the generalized-α scheme for
constrained mechanical systems[J]. Multibody System Dynamics, 2007, 18:
185-202.</p>
</blockquote>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://wjq5712.github.io/2023/06/10/%E7%9F%A9%E9%98%B5%E7%9A%84%E5%AF%BC%E6%95%B0%E8%BF%90%E7%AE%97/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Wu Jiaqi">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/06/10/%E7%9F%A9%E9%98%B5%E7%9A%84%E5%AF%BC%E6%95%B0%E8%BF%90%E7%AE%97/" class="post-title-link" itemprop="url">矩阵的导数运算</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-06-10 16:58:07" itemprop="dateCreated datePublished" datetime="2023-06-10T16:58:07+08:00">2023-06-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-06-12 17:29:36" itemprop="dateModified" datetime="2023-06-12T17:29:36+08:00">2023-06-12</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="矩阵的导数运算1"><a
target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1av4y1b7MM/?spm_id_from=333.1007.top_right_bar_window_history.content.click">矩阵的导数运算</a></h1>
<h2 id="标量方程对向量的导数">1. 标量方程对向量的导数</h2>
<p>假设一个标量方程： <span class="math display">\[
y=f(\pmb{x})
\]</span> 其中，<span class="math inline">\(\boldsymbol{x}=[x_1\quad x_2
\quad \cdots \quad x_n]^T\)</span>。</p>
<p><span class="math inline">\(y\)</span>对<span
class="math inline">\(\pmb{x}\)</span>的导数有分母布局和分子布局两种形式。</p>
<p>分母布局（Denominator layout），偏导的行数与分母相同： <span
class="math display">\[
\frac{\partial y}{\partial \pmb{x}}=
\begin{bmatrix}
\frac{\partial y}{\partial x_1} \\
\frac{\partial y}{\partial x_2} \\
\vdots \\
\frac{\partial y}{\partial x_n}
\end{bmatrix}
\]</span></p>
<p>分子布局（Numerator layout），偏导的行数与分子相同：</p>
<p><span class="math display">\[
\frac{\partial y}{\partial \pmb{x}}=
\begin{bmatrix}
\frac{\partial y}{\partial x_1} \quad
\frac{\partial y}{\partial x_2} \quad
\cdots \quad
\frac{\partial y}{\partial x_n}
\end{bmatrix}
\]</span> 分子布局与分母布局的结果互为转置。</p>
<h2 id="标量方程对矩阵的导数">2. 标量方程对矩阵的导数</h2>
<p>假设一个标量方程： <span class="math display">\[
y=f(\pmb{X})
\]</span> 其中， <span class="math display">\[
\pmb{X}=
\begin{bmatrix}
X_{11} \quad X_{12} \quad \dots \quad X_{1n} \\
X_{21} \quad X_{22} \quad \dots \quad X_{2n} \\
\vdots
X_{m1} \quad X_{m2} \quad \dots \quad X_{mn} \\
\end{bmatrix}
\]</span> 则<span class="math inline">\(y\)</span>对<span
class="math inline">\(\pmb{X}\)</span>的导数为： <span
class="math display">\[
\
\]</span></p>
<h2 id="向量方程对向量的导数">3. 向量方程对向量的导数</h2>
<p>假设一个向量方程： <span class="math display">\[
\pmb{y} = f(\pmb{x})
\]</span> 其中，<span class="math inline">\(\pmb{x}=[x_1\quad x_2 \quad
\cdots \quad x_n]^T\)</span>，<span class="math inline">\(\pmb{y}=[y_1
\quad y_2 \quad \cdots \quad y_m]^T\)</span>。</p>
<p>分母布局下，<span class="math inline">\(\pmb{y}\)</span>对<span
class="math inline">\(\pmb{x}\)</span>的导数为： <span
class="math display">\[
\frac{\partial \pmb{y}_{m \times 1}}{\partial \pmb{x}_{n \times 1}}=
\begin{bmatrix}
\frac{\partial \pmb{y}}{\partial x_1} \\
\frac{\partial \pmb{y}}{\partial x_2} \\
\vdots \\
\frac{\partial \pmb{y}}{\partial x_n} \\
\end{bmatrix}
=
\begin{bmatrix}
\frac{\partial y_1}{\partial x_1} \quad \frac{\partial y_2}{\partial
x_1} \quad \dots \quad \frac{\partial y_m}{\partial x_1} \\
\frac{\partial y_1}{\partial x_2} \quad \frac{\partial y_2}{\partial
x_2} \quad \dots \quad \frac{\partial y_m}{\partial x_2} \\
\vdots \\
\frac{\partial y_1}{\partial x_n} \quad \frac{\partial y_2}{\partial
x_n} \quad \dots \quad \frac{\partial y_m}{\partial x_n} \\
\end{bmatrix}
\]</span> 第一步先使<span
class="math inline">\(\pmb{y}\)</span>分别对<span
class="math inline">\(x_1\dots
\,x_n\)</span>求导，由于采用分母布局，因此结果为纵向展开，行数为<span
class="math inline">\(n\)</span>；第二步在对每一行继续按照分母布局展开，每一行的展开结果仍然是1行，列数为<span
class="math inline">\(m\)</span>。综上，分母布局下的最终求导结果为一个<span
class="math inline">\(m \times n\)</span>的矩阵。</p>
<p>分子布局同理，结果为分母布局的转置。结果如下： <span
class="math display">\[
\frac{\partial \pmb{y}_{m \times 1}}{\partial \pmb{x}_{n \times 1}}=
\begin{bmatrix}
\frac{\partial \pmb{y}}{\partial x_1} \\
\frac{\partial \pmb{y}}{\partial x_2} \\
\vdots \\
\frac{\partial \pmb{y}}{\partial x_n} \\
\end{bmatrix}
=
\begin{bmatrix}
\frac{\partial y_1}{\partial x_1} \quad \frac{\partial y_1}{\partial
x_2} \quad \dots \quad \frac{\partial y_1}{\partial x_n} \\
\frac{\partial y_2}{\partial x_1} \quad \frac{\partial y_2}{\partial
x_2} \quad \dots \quad \frac{\partial y_2}{\partial x_n} \\
\vdots \\
\frac{\partial y_m}{\partial x_1} \quad \frac{\partial y_m}{\partial
x_2} \quad \dots \quad \frac{\partial y_m}{\partial x_n} \\
\end{bmatrix}
\]</span> 常用特例：</p>
<p>在分母布局下，假设<span class="math inline">\(\pmb{x}=[x_1 \quad x_2
\quad \cdots \quad x_m]^T\)</span>，且 <span class="math display">\[
\pmb{A}=
\begin{bmatrix}
a_{11} \quad a_{12} \quad \dots \quad a_{1m} \\
a_{21} \quad a_{22} \quad \dots \quad a_{2m} \\
\vdots \\
a_{m1} \quad a_{m2} \quad \dots \quad a_{mm} \\
\end{bmatrix}
\]</span></p>
<p>则 <span class="math display">\[
\frac{\partial \pmb{A} \pmb{x}}{\partial \pmb{x}} = \pmb{A}^\text{T}
\]</span> 而在<strong>分子布局</strong>下 <span class="math display">\[
\frac{\partial \pmb{A} \pmb{x}}{\partial \pmb{x}} = \pmb{A}
\]</span></p>
<p>对二次型有： <span class="math display">\[
\frac{\partial \pmb{x}^\text{T} \pmb{A} \pmb{x}}{\partial
\pmb{x}}=\pmb{A}\pmb{x}+\pmb{A}^\text{T}\pmb{x}
\]</span> 特别的，当<span
class="math inline">\(A\)</span>为对称矩阵时，<span
class="math inline">\(\frac{\partial \pmb{x}^T \pmb{A} \pmb{x}}{\partial
\pmb{x}}=2 \pmb{A}\pmb{x}\)</span>。</p>
<p><strong>注意：在公式推导的过程中必须要确定使用分母布局还是分子布局，混用两种布局会在编程仿真中出现矩阵维度不匹配的问题！！！</strong></p>
<h2 id="矩阵求导的链式法则2">3. <a
target="_blank" rel="noopener" href="https://www.cnblogs.com/pinard/p/10825264.html">矩阵求导的链式法则</a></h2>
<p><strong>约定：标量函数对向量、矩阵求导采用分母布局，向量函数对向量求导采用分子布局。</strong></p>
<p><strong>前者是为了保持求导结果与自变量向量维度相同，后者是为了与雅各比矩阵定义一致。</strong></p>
<h3 id="向量函数对向量求导的链式法则">3.1
向量函数对向量求导的链式法则</h3>
<p>假设多个向量存在依赖关系，比如三个向量<span
class="math inline">\(\pmb{x} \rightarrow \pmb{y} \rightarrow
\pmb{z}\)</span>，则有下面的链式求导法则： <span class="math display">\[
\frac{\partial \pmb{z}}{\partial \pmb{x}} = \frac{\partial
\pmb{z}}{\partial \pmb{y}} \frac{\partial \pmb{y}}{\partial \pmb{x}}
\]</span>
该法则可以推广到更多的向量依赖关系，但要注意依赖关系中所有的变量都是向量，如果其中存在矩阵或者标量，则该链式法则不成立。</p>
<p>假设有n个向量满足依赖关系：<span class="math inline">\(\pmb{y_1}
\rightarrow \pmb{y_1} \rightarrow \dots \rightarrow
\pmb{y_n}\)</span>，则有下面的链式求导法则： <span
class="math display">\[
\frac{\partial \pmb{y_n}}{\partial \pmb{y_1}} = \frac{\partial
\pmb{y_n}}{\partial \pmb{y_{n-1}}} \frac{\partial
\pmb{y_{n-1}}}{\partial \pmb{y_{n-1}}} \dots \frac{\partial
\pmb{y_2}}{\partial \pmb{y_1}}
\]</span></p>
<h3 id="标量函数对向量求导的链式法则">3.2
标量函数对向量求导的链式法则</h3>
<p>假设有一个标量<span class="math inline">\(z\)</span>和两个向量<span
class="math inline">\(\pmb{x}、\pmb{y}\)</span>，它们满足依赖关系：<span
class="math inline">\(\pmb{x} \rightarrow \pmb{y} \rightarrow
z\)</span>，则有下面的链式求导法则： <span class="math display">\[
\frac{\partial z}{\partial \pmb{x}} = (\frac{\partial \pmb{y}}{\partial
\pmb{x}})^\text{T} \frac{\partial z}{\partial \pmb{y}}
\]</span> 如果是标量函数对多个向量求导，比如：<span
class="math inline">\(\pmb{y_1} \rightarrow \pmb{y_1} \rightarrow \dots
\rightarrow \pmb{y_n} \rightarrow z\)</span>，则其链式法则可表示为：
<span class="math display">\[
\frac{\partial z}{\partial \pmb{y_1}} = (\frac{\partial
\pmb{y_n}}{\partial \pmb{y_{n-1}}} \frac{\partial
\pmb{y_{n-1}}}{\partial \pmb{y_{n-1}}} \dots \frac{\partial
\pmb{y_2}}{\partial \pmb{y_1}})^\text{T} \frac{\partial z}{\partial
\pmb{y_n}}
\]</span></p>
<p>## 4. 矩阵导数的乘法公式</p>
<h3 id="标量向量对标量求导">4.1 标量×向量对标量求导</h3>
假设有一个向量<span
class="math inline">\(\pmb{y}=u(x)\pmb{v}(x)\)</span>，其中<span
class="math inline">\(u(x)\)</span>是关于标量<span
class="math inline">\(x\)</span>的标量函数，<span
class="math inline">\(\pmb{v}(x)\)</span>是关于标量<span
class="math inline">\(x\)</span>的向量函数，则有如下求导公式： $$ {x} =
=
<span class="math display">\[\begin{bmatrix}
\frac{\partial u}{\partial x} v_1 + u \frac{\partial v_1}{\partial x} \\
\vdots \\
\frac{\partial u}{\partial x} v_m + u \frac{\partial v_m}{\partial x}
\end{bmatrix}\]</span>
<p>= + u $$</p>
<h3 id="标量向量对向量求导">4.2 标量×向量对向量求导</h3>
<p>假设有一个向量<span
class="math inline">\(\pmb{y}=u(\pmb{x})\pmb{v}(\pmb{x})\)</span>，其中<span
class="math inline">\(u(\pmb{x})\)</span>是关于标量<span
class="math inline">\(\pmb{x}\)</span>的向量函数，<span
class="math inline">\(\pmb{v}(\pmb{x})\)</span>是关于向量<span
class="math inline">\(\pmb{x}\)</span>的向量函数，则有如下求导公式：
<span class="math display">\[
\frac{\partial \pmb{y}} {\partial \pmb{x} }
=
\begin {bmatrix}
\frac{\partial y_1}{\partial x_1} \dots \frac{\partial y_1}{\partial
x_n} \\
\vdots \quad  \quad \quad  \vdots\\
\frac{\partial y_m}{\partial x_n} \dots \frac{\partial y_m}{\partial
x_n}
\end{bmatrix}
=
\begin{bmatrix}
\frac{\partial u}{\partial x_1} v_1 + u \frac{\partial v_1}{\partial
x_1} \dots \frac{\partial u}{\partial x_n} v_1 + u \frac{\partial
v_1}{\partial x_n}\\
\vdots \quad  \quad \quad \quad \quad \quad \quad  \vdots\\
\frac{\partial u}{\partial x_1} v_m + u \frac{\partial v_m}{\partial
x_1} \dots \frac{\partial u}{\partial x_n} v_m + u \frac{\partial
v_m}{\partial x_n}\\
\end{bmatrix}
=
\pmb{v} (\frac{\partial u}{\partial \pmb{x}})^\text{T} + u
\frac{\partial \pmb{v}}{\partial \pmb{x}}
\]</span></p>
<h2 id="矩阵导数的除法公式">5. 矩阵导数的除法公式</h2>
<h3 id="标量向量对标量求导-1">5.1 标量/向量对标量求导</h3>
假设有一个向量<span
class="math inline">\(\pmb{y}=\pmb{u}({x})/{v}({x})\)</span>，其中<span
class="math inline">\(\pmb{u}(x)\)</span>是关于标量<span
class="math inline">\({x}\)</span>的向量函数，<span
class="math inline">\({v}({x})\)</span>是关于标量<span
class="math inline">\({x}\)</span>的标量函数，则有如下求导公式： $$ {x}
=
=
<span class="math display">\[\begin{bmatrix}
\frac{1}{v^2}(\frac{\partial u}{\partial x} v_1 - u \frac{\partial
v_1}{\partial x}) \\
\vdots \\
\frac{1}{v^2}(\frac{\partial u}{\partial x} v_m - u \frac{\partial
v_m}{\partial x})
\end{bmatrix}\]</span>
<p>= ( - u ) $$</p>
<h3 id="标量向量对向量求导-1">5.2 标量/向量对向量求导</h3>
<p>假设有一个向量<span
class="math inline">\(\pmb{y}=\pmb{u}(\pmb{x})/{v}(\pmb{x})\)</span>，其中<span
class="math inline">\(\pmb{u}(\pmb{x})\)</span>是关于向量<span
class="math inline">\(\pmb{x}\)</span>的向量函数，<span
class="math inline">\({v}(\pmb{x})\)</span>是关于向量<span
class="math inline">\(\pmb{x}\)</span>的标量函数，则有如下求导公式：
<span class="math display">\[
\begin{aligned}
\frac{\partial \pmb{y}} {\partial \pmb{x} }
&amp;=
\begin {bmatrix}
\frac{\partial y_1}{\partial x_1} \dots \frac{\partial y_1}{\partial
x_n} \\
\vdots \quad  \quad \quad  \vdots\\
\frac{\partial y_m}{\partial x_n} \dots \frac{\partial y_m}{\partial
x_n}
\end{bmatrix}
=
\begin{bmatrix}
\frac{1}{v^2}(\frac{\partial u}{\partial x_1} v_1 - u \frac{\partial
v_1}{\partial x_1})
\dots
\frac{1}{v^2}(\frac{\partial u}{\partial x_n} v_1 - u \frac{\partial
v_1}{\partial x_n}) \\
\vdots \quad  \quad \quad \quad \quad \quad \quad  \vdots\\
\frac{1}{v^2}(\frac{\partial u}{\partial x_1} v_m - u \frac{\partial
v_m}{\partial x_1})
\dots
\frac{1}{v^2}(\frac{\partial u}{\partial x_n} v_m - u \frac{\partial
v_m}{\partial x_n}) \\
\end{bmatrix}\\
\\
&amp;=
\frac{1}{v^2}({v}\frac{\partial \pmb{u}}{\partial \pmb{x}} - \pmb{u}
(\frac{\partial {v}}{\partial \pmb{x}})^\text{T})
\end{aligned}
\]</span></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://wjq5712.github.io/2023/06/10/%E5%A7%BF%E6%80%81%E5%8A%A8%E5%8A%9B%E5%AD%A6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Wu Jiaqi">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/06/10/%E5%A7%BF%E6%80%81%E5%8A%A8%E5%8A%9B%E5%AD%A6/" class="post-title-link" itemprop="url">姿态动力学及控制</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2023-06-10 15:58:07 / Modified: 16:34:25" itemprop="dateCreated datePublished" datetime="2023-06-10T15:58:07+08:00">2023-06-10</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="姿态动力学及控制">姿态动力学及控制</h1>
<h2 id="基于四元数的姿态动力学">1 基于四元数的姿态动力学</h2>
<h3 id="四元数的定义">1.1 四元数的定义</h3>
<p>任意两个坐标系之间的姿态转换关系都可以通过绕空间中一根轴旋转一个角度来描述，假设这根轴用一个单位向量<span
class="math inline">\([e_1 \quad e_2 \quad
e_3]^\text{T}\)</span>来表示，旋转的角度为<span
class="math inline">\(\theta\)</span>，<strong>四元数</strong>（又名欧拉参数），定义如下：
<span class="math display">\[
\begin{aligned}
&amp; q_0 = e_1sin(\theta/2) \\
&amp; q_1 = e_2sin(\theta/2) \\
&amp; q_2 = e_3sin(\theta/2) \\
&amp; q_3 = cos(\theta/2) \\
\end{aligned}
\]</span> 四元数<span class="math inline">\(\pmb{q}=[q_0 \quad q_1 \quad
q_2 \quad q_3]^\text{T}\)</span>，其中，<span
class="math inline">\(q_0\)</span>为标量部分，<span
class="math inline">\(\pmb{q}_r=[q_1 \quad q_2 \quad
q_3]^\text{T}\)</span>为矢量部分。</p>
<h3 id="四元数描述的运动学">1.2 四元数描述的运动学</h3>
<p>标量形式： <span class="math display">\[
\begin{bmatrix}
\dot{q}_0 \\ \dot{q}_1 \\ \dot{q}_2 \\ \dot{q}_3 \\
\end{bmatrix}
=
\frac{1}{2}
\begin{bmatrix}
&amp;0 \quad &amp;-\omega_1 \quad &amp;-\omega_2 \quad &amp;-\omega_3 \\
&amp;\omega_1 \quad &amp;0 \quad &amp;\omega_3 \quad &amp;-\omega_2 \\
&amp;\omega_2 \quad &amp; -\omega_3 \quad &amp;0 \quad &amp;\omega_1 \\
&amp;\omega_3 \quad &amp; \omega_2 \quad &amp;-\omega_1 \quad &amp;0
\end{bmatrix}
\begin{bmatrix}
{q}_0 \\ {q}_1 \\ {q}_2 \\ {q}_3 \\
\end{bmatrix}
\]</span> 矢量形式： <span class="math display">\[
\begin{aligned}
&amp; \dot q _0 = -\frac{1}{2} \pmb{\omega}^\text{T}\pmb{q}_r \\
&amp; \dot{\pmb{q}}_r = \frac{1}{2}
(q_0\pmb{\omega}-\pmb{\omega}^{\times}\pmb{q}_r)
\end{aligned}
\]</span> 其中， <span class="math display">\[
\pmb{\omega}^{\times} =
\begin{bmatrix}
&amp; 0 \quad &amp; -\omega_3 \quad &amp; \omega_2 \\
&amp; \omega_3 \quad &amp; 0 \quad &amp; -\omega_1 \\
&amp; -\omega_2 \quad &amp; \omega_1 \quad &amp; 0
\end{bmatrix}
\]</span></p>
<h3 id="四元数描述的运动学-1">1.3 四元数描述的运动学</h3>
<p><span class="math display">\[
\pmb{T} = \pmb{I}\dot{\pmb{\omega}}+\pmb{\omega}^{\times}\pmb{I\omega}
\]</span></p>
<p>其中，<span
class="math inline">\(\pmb{T}\)</span>为刚体受到的合外力矩，<span
class="math inline">\(\pmb{\omega}\)</span>为刚体角速度（陀螺仪测得的本体系相对惯性系的角速度在本体系下的分量），<span
class="math inline">\(\pmb{I}\)</span>为刚体的转动惯量矩阵。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>





</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Wu Jiaqi</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  






  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
